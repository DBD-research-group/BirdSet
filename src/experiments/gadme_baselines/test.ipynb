{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/miniconda3/envs/GADME/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "import torch\n",
    "import torch_audiomentations\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "\n",
    "from transformers import BatchFeature\n",
    "from transformers import SequenceFeatureExtractor\n",
    "from transformers.utils import PaddingStrategy\n",
    "import numpy as np \n",
    "from transformers import AutoFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"DBD-research-group/gadme_v1_1\", \"NA_subset500\", cache_dir=\"~/data_gadme\")\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0, 'test': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ds.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.cast_column(\n",
    "    column=\"audio\",\n",
    "    feature=Audio(\n",
    "        sampling_rate=16_000,\n",
    "        mono=True,\n",
    "        decode=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch):\n",
    "    feature_extractor = extractor\n",
    "    audio_arrays = [x[\"array\"] for x in batch[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=16_000,\n",
    "        padding=True,\n",
    "        max_length=16_000*1,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\"],\n",
    "    batched=True,\n",
    "    batch_size=50,\n",
    "    load_from_cache_file=True,\n",
    "    num_proc=3,\n",
    ")\n",
    "ds = ds.select_columns([\"input_values\",\"ebird_code\"])\n",
    "\n",
    "ds = ds.rename_column(\"ebird_code\", \"labels\")\n",
    "\n",
    "ds.set_format(\"np\")\n",
    "\n",
    "split=ds[\"train\"].train_test_split(0.1, shuffle=True, seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #cache_dir: ~/data_gadme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'call_type', 'sex', 'lat', 'long', 'microphone', 'licence', 'source', 'local_time', 'detected_events', 'quality', 'input_values'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'call_type', 'sex', 'lat', 'long', 'microphone', 'licence', 'source', 'local_time', 'detected_events', 'quality', 'input_values'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'call_type', 'sex', 'lat', 'long', 'microphone', 'licence', 'source', 'local_time', 'detected_events', 'quality', 'input_values'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filepath', 'start_time', 'end_time', 'low_freq', 'high_freq', 'ebird_code', 'call_type', 'sex', 'lat', 'long', 'microphone', 'licence', 'source', 'local_time', 'detected_events', 'quality', 'input_values'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': None,\n",
       " 'array': array([ 0.00061898, -0.00345948, -0.00828452, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " 'sampling_rate': 32000}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': None,\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['audio',\n",
       "  'filepath',\n",
       "  'start_time',\n",
       "  'end_time',\n",
       "  'low_freq',\n",
       "  'high_freq',\n",
       "  'ebird_code',\n",
       "  'call_type',\n",
       "  'sex',\n",
       "  'lat',\n",
       "  'long',\n",
       "  'microphone',\n",
       "  'licence',\n",
       "  'source',\n",
       "  'local_time',\n",
       "  'detected_events',\n",
       "  'quality'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.cast_column(\n",
    "            column=\"audio\",\n",
    "            feature=Audio(\n",
    "                sampling_rate=32_000,\n",
    "                mono=True,\n",
    "                decode=True\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': None,\n",
       " 'array': array([ 0.00061898, -0.00345948, -0.00828452, ...,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " 'sampling_rate': 32000}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeatureExtractor(SequenceFeatureExtractor):\n",
    "    model_input_names = [\"input_values\", \"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_size=1,\n",
    "        sampling_rate=32_000,\n",
    "        padding_value=0.0,\n",
    "        return_attention_mask=False,\n",
    "        do_normalize=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # initialize sequencefeatureextractor\n",
    "        super().__init__(feature_size=feature_size, sampling_rate=sampling_rate, padding_value=padding_value)\n",
    "        self.return_attention_mask = return_attention_mask\n",
    "        self.do_normalize = do_normalize\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(\n",
    "        input_values,\n",
    "        attention_mask,\n",
    "        padding_value=0.0\n",
    "    ):\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = np.array(attention_mask, np.int32)\n",
    "            normed_input_values = []\n",
    "\n",
    "            for vector, length in zip(input_values, attention_mask.sum(-1)):\n",
    "                normed_slice = (vector - vector[:length].mean()) / np.sqrt(vector[:length].var() + 1e-7)\n",
    "                if length < normed_slice.shape[0]:\n",
    "                    normed_slice[length:] = padding_value\n",
    "\n",
    "                normed_input_values.append(normed_slice)\n",
    "        else:\n",
    "            normed_input_values = [(x-x.mean()) / np.sqrt(x.var() + 1e-7) for x in input_values]\n",
    "        return normed_input_values\n",
    "    \n",
    "    def __call__(\n",
    "        self, \n",
    "        raw_audio,\n",
    "        padding = False, \n",
    "        max_length = None,\n",
    "        truncation = False, \n",
    "        return_tensors = None,\n",
    "        sampling_rate = None,\n",
    "        return_attention_mask = None,\n",
    "        **kwargs\n",
    "    ) -> BatchFeature:\n",
    "        \n",
    "        # control/check sampling rate \n",
    "        if self.sampling_rate is not None: \n",
    "            if sampling_rate != self.sampling_rate:\n",
    "                raise ValueError(\n",
    "                    f\"The model corresponding to this feature extractor: {self} was trained using a sampling rate of\"\n",
    "                    f\"{self.sampling_rate}. Make sure that the provided `raw_audio`input was sampled with\"\n",
    "                    f\"{self.sampling_rate} and not {sampling_rate}.\"\n",
    "                )\n",
    "        else:\n",
    "            print( \"It is strongly recommended to pass the ``sampling_rate`` argument to this function. \\\n",
    "                    Failing to do so can result in silent errors that might be hard to debug.\")\n",
    "        # check batch input\n",
    "        is_batched_numpy = isinstance(raw_audio, np.ndarray) and len(raw_audio.shape) > 1\n",
    "        is_batched = is_batched_numpy or (\n",
    "            isinstance(raw_audio, (list, tuple)) and (isinstance(raw_audio[0], (np.ndarray, tuple, list)))\n",
    "        )\n",
    "\n",
    "        if not is_batched:\n",
    "            raw_audio = [raw_audio]\n",
    "\n",
    "        encoded_inputs = BatchFeature({\"input_values\": raw_audio})\n",
    "\n",
    "        padded_inputs = self.pad(\n",
    "            encoded_inputs,\n",
    "            padding=padding,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "            truncation=truncation\n",
    "        )\n",
    "\n",
    "        #convert into format \n",
    "        input_values = padded_inputs[\"input_values\"]\n",
    "        if not isinstance(input_values[0], np.ndarray):\n",
    "            padded_inputs[\"input_values\"] = [np.asarray(array, dtype=np.float32) for array in input_values]\n",
    "        elif (\n",
    "            not isinstance(input_values, np.ndarray)\n",
    "            and isinstance(input_values[0], np.ndarray)\n",
    "            and input_values[0].dtype is np.dtype(np.float64)\n",
    "        ):\n",
    "            padded_inputs[\"input_values\"] = [array.astype(np.float32) for array in input_values]\n",
    "        elif isinstance(input_values, np.ndarray) and input_values.dtype is np.dtype(np.float64):\n",
    "            padded_inputs[\"input_values\"] = input_values.astype(np.float32)\n",
    "        # return_to_tensors comes from: transformers/src/transformers/feature_extraction_utils.py\n",
    "        if return_tensors is not None:\n",
    "            padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n",
    "        \n",
    "        attention_mask = padded_inputs.get(\"attention_mask\")\n",
    "        if attention_mask is not None: \n",
    "            padded_inputs[\"attention_mask\"] = [np.asarray(array, dtype=np.int32) for array in attention_mask]\n",
    "        \n",
    "        if self.do_normalize:\n",
    "            attention_mask = (\n",
    "                attention_mask\n",
    "                if self._get_padding_strategies(padding, max_length=max_length) is not PaddingStrategy.DO_NOT_PAD\n",
    "                else None\n",
    "            )\n",
    "            padded_inputs[\"input_values\"] = self.normalize(\n",
    "                padded_inputs[\"input_values\"], attention_mask=attention_mask, padding_value=self.padding_value\n",
    "            )\n",
    "  \n",
    "        return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = CustomFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(samples):\n",
    "    audio_arrays = [x[\"array\"] for x in samples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        padding=True,\n",
    "        max_length=32_000*1,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\"],\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    load_from_cache_file=True,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': None,\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['filepath',\n",
       "  'start_time',\n",
       "  'end_time',\n",
       "  'low_freq',\n",
       "  'high_freq',\n",
       "  'ebird_code',\n",
       "  'call_type',\n",
       "  'sex',\n",
       "  'lat',\n",
       "  'long',\n",
       "  'microphone',\n",
       "  'licence',\n",
       "  'source',\n",
       "  'local_time',\n",
       "  'detected_events',\n",
       "  'quality',\n",
       "  'input_values'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'torch',\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['filepath',\n",
       "  'start_time',\n",
       "  'end_time',\n",
       "  'low_freq',\n",
       "  'high_freq',\n",
       "  'ebird_code',\n",
       "  'call_type',\n",
       "  'sex',\n",
       "  'lat',\n",
       "  'long',\n",
       "  'microphone',\n",
       "  'licence',\n",
       "  'source',\n",
       "  'local_time',\n",
       "  'detected_events',\n",
       "  'quality',\n",
       "  'input_values'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gadme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
