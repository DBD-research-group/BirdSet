{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Benchmark models: ResNet"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "vscode": {
                    "languageId": "plaintext"
                }
            },
            "source": [
                "# Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "plaintext"
                },
                "pycharm": {
                    "is_executing": true
                }
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import torch\n",
                "\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "from torchaudio import transforms\n",
                "\n",
                "import pytorch_lightning as pl\n",
                "from pytorch_lightning.callbacks import RichProgressBar\n",
                "\n",
                "import IPython.display as ipd"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Augmentation module for our datapipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from experiments.models.benchmark_models import LightningResNet\n",
                "from src.utils.preprocess import collate_batch, create_dataset, undo_standardize_tensor"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Define configuration file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config_datapipeline = {\"use_spectrogram\": True,\n",
                "                       \"n_fft\": 1024, # The height of the spectrogram will be n_fft // 2 + 1\n",
                "                       \"hop_length\": 512, # The number of frames in the spectrogram (i.e. it's width) will be (audio_length - n_fft) // hop_length + 1, where audio_length is the length of the audio signal.\n",
                "                       \"n_mels\": None,\n",
                "                       \"waveform_augmentations\": {\"colored_noise\": {\"prob\": 0.5, \"min_snr_in_db\": 3.0, \"max_snr_in_db\": 30.0,\n",
                "                                                                    \"min_f_decay\": -2.0, \"max_f_decay\": 2.0,},\n",
                "                                                  #\"background_noise\": {\"background_paths\": \"/mnt/home/rheinrich/deep_bird_detect/datapipeline/speech_command_dataset\" ,\"prob\":0.5,\n",
                "                                                  #                     \"min_snr_in_db\": 3.0, \"max_snr_in_db\": 30.0,},\n",
                "                                                  #\"pitch_shift\": {\"prob\": 0.5, \"min_transpose_semitones\": -4.0, \"max_transpose_semitones\": 4.0,},\n",
                "                                                 },\n",
                "                       \"spectrogram_augmentations\": {\"time_masking\": {\"time_mask_param\": 100, \"prob\": 0.5},\n",
                "                                                     \"frequency_masking\": {\"freq_mask_param\": 100, \"prob\": 0.5},\n",
                "                                                     \"time_stretch\": {\"prob\": 0.5, \"min_rate\": 0.8, \"max_rate\": 1.2}},\n",
                "                       \"learning_rate\": 0.01,\n",
                "                       \"batch_size\": 64,\n",
                "                       \"num_workers\": 32,\n",
                "                      }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create a HuggingFace dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Here, we use the ESC50 dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataset, train_push_dataset, test_dataset, train_mean, train_std = create_dataset(path = \"ashraq/esc50\",\n",
                "                                                                                        split = \"train\",\n",
                "                                                                                        columns = [\"audio\", \"target\"],\n",
                "                                                                                        test_size = 0.2,\n",
                "                                                                                        use_spectrogram = config_datapipeline[\"use_spectrogram\"],\n",
                "                                                                                        waveform_augmentations = config_datapipeline[\"waveform_augmentations\"],\n",
                "                                                                                        spectrogram_augmentations =config_datapipeline[\"spectrogram_augmentations\"],\n",
                "                                                                                        n_fft = config_datapipeline[\"n_fft\"],\n",
                "                                                                                        hop_length = config_datapipeline[\"hop_length\"],\n",
                "                                                                                        n_mels = config_datapipeline[\"n_mels\"],\n",
                "                                                                                       )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Show transformations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Input values & targets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axs = plt.subplots(3,3, figsize=(20, 30))\n",
                "axs = axs.flatten()\n",
                "print(axs.shape)\n",
                "examples = train_dataset[:9]\n",
                "spectrograms = examples[\"input_values\"]\n",
                "labels = examples[\"target\"]\n",
                "for spectrogram, label, ax in zip(spectrograms, labels, axs):\n",
                "    ax.set_title(label)\n",
                "    im = ax.imshow(spectrogram.squeeze().numpy())\n",
                "    fig.colorbar(im, ax = ax)   "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Convert one spectrogram back to waveform audio data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The GriffinLim transformation converts power spectrograms back to waveform."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If Mel-scale spectrograms are used instead, the InverseMelScale transformation has to be applied first, see https://pytorch.org/audio/stable/transforms.html."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_fft = config_datapipeline[\"n_fft\"]\n",
                "hop_length=config_datapipeline[\"hop_length\"]\n",
                "#n_stft= n_fft // 2 + 1\n",
                "#n_mels=config_datapipeline[\"n_mels\"]\n",
                "#sample_rate=train_dataset[0][\"audio\"][\"sampling_rate\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The waveform files from InverseMelScale have very poor quality and long runtimes, so we should try to work with power spectrograms instead of mel spectrograms if possible!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#transform_to_waveform = torchvision.transforms.Compose([transforms.InverseMelScale(n_stft=n_stft, n_mels=n_mels, sample_rate=sample_rate, max_iter=10000), transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transform_to_waveform = transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Select one sample from the training dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "example = train_dataset[103]\n",
                "example_spectrogram = example[\"input_values\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Convert the spectrogram to a waveform"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# undo z standardization\n",
                "example_spectrogram_denormalized = undo_standardize_tensor(x=example_spectrogram.unsqueeze(0), mean=train_mean, std=train_std)\n",
                "\n",
                "# squeeze batch dimension\n",
                "example_spectrogram_denormalized = example_spectrogram_denormalized.squeeze(0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note: since the spectrogram was log10 transformed in the AudioAugmentor, we also need to invert the log transformation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# undo log10 transformation\n",
                "example_spectrogram_denormalized = torch.pow(10, example_spectrogram_denormalized)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "example_waveform = transform_to_waveform(example_spectrogram_denormalized)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Augmented spectrogram"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(example_spectrogram.squeeze().numpy())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### original waveform audio data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "original_waveform = example[\"audio\"][\"array\"]\n",
                "sample_rate = example[\"audio\"][\"sampling_rate\"]\n",
                "ipd.Audio(data=original_waveform, rate=sample_rate)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### the waveform obtained from the augmented spectrogram."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ipd.Audio(data=example_waveform, rate=sample_rate)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create a PyTorch dataloader from the HuggingFace dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataloader = DataLoader(train_dataset,\n",
                "                              batch_size=config_datapipeline[\"batch_size\"], \n",
                "                              num_workers=config_datapipeline[\"num_workers\"],\n",
                "                              shuffle=True, \n",
                "                              collate_fn = lambda x: collate_batch(x, return_category=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_dataloader = DataLoader(test_dataset,\n",
                "                             batch_size=config_datapipeline[\"batch_size\"],\n",
                "                             num_workers=config_datapipeline[\"num_workers\"],\n",
                "                             shuffle=False, \n",
                "                             collate_fn = lambda x: collate_batch(x, return_category=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Train a benchmark model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create Lightning Data Module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LitDataModule(pl.LightningDataModule):\n",
                "    def __init__(self,):\n",
                "        super().__init__()\n",
                "        \n",
                "    def setup(self, stage=None):\n",
                "        pass\n",
                "\n",
                "    def train_dataloader(self):\n",
                "        return train_dataloader\n",
                "    \n",
                "    def val_dataloader(self):\n",
                "        return test_dataloader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Initiate Lightning Data Module"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datamodule = LitDataModule()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define benchmark model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train Model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Callbacks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
                "    monitor= 'val_loss',\n",
                "    dirpath='./saved_models/benchmark/resnet18/',\n",
                "    filename='best_resnet18_model',\n",
                "    save_top_k=1,\n",
                "    mode='min')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_loss',\n",
                "                                            patience = 15)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Initiate Model and Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = LightningResNet(baseline_architecture=\"resnet18\",\n",
                "                        num_classes=50, \n",
                "                        num_channels=1,\n",
                "                        learning_rate= 0.01,)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer = pl.Trainer(max_epochs= 100,\n",
                "                     accelerator='gpu',\n",
                "                     devices=1,\n",
                "                     callbacks=[checkpoint_callback, early_stopping, pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-2), RichProgressBar()],\n",
                "                     log_every_n_steps=5\n",
                "                    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Start Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit model\n",
                "trainer.fit(model, datamodule = datamodule)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load best Checkpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = model.load_from_checkpoint('./saved_models/benchmark/resnet18/best_resnet18_model.ckpt', \n",
                "                                   baseline_architecture=\"resnet18\",\n",
                "                                   num_classes=50,\n",
                "                                   num_channels=1,\n",
                "                                   learning_rate= config_datapipeline[\"learning_rate\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Validate model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Validation set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.validate(model, dataloaders = datamodule)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "language": "python",
            "display_name": "Python 3 (ipykernel)"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}