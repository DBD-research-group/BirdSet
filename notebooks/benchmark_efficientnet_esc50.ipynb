{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Benchmark models: EfficientNet"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "vscode": {
               "languageId": "plaintext"
            }
         },
         "source": [
            "# Imports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "vscode": {
               "languageId": "plaintext"
            }
         },
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "import torch\n",
            "\n",
            "from torch.utils.data import DataLoader\n",
            "\n",
            "from torchaudio import transforms\n",
            "\n",
            "import pytorch_lightning as pl\n",
            "from pytorch_lightning.callbacks import RichProgressBar\n",
            "\n",
            "import IPython.display as ipd"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Augmentation module for our datapipeline"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from experiments.models.benchmark_models import LightningEfficientNet\n",
            "from src.utils.preprocess import collate_batch, create_dataset, undo_standardize_tensor"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Define configuration file"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "config_datapipeline = {\"use_spectrogram\": True,\n",
            "                       \"n_fft\": 1024, # The height of the spectrogram will be n_fft // 2 + 1\n",
            "                       \"hop_length\": 512, # The number of frames in the spectrogram (i.e. it's width) will be (audio_length - n_fft) // hop_length + 1, where audio_length is the length of the audio signal.\n",
            "                       \"n_mels\": None,\n",
            "                       \"waveform_augmentations\": {\"colored_noise\": {\"prob\": 0.5, \"min_snr_in_db\": 3.0, \"max_snr_in_db\": 30.0,\n",
            "                                                                    \"min_f_decay\": -2.0, \"max_f_decay\": 2.0,},\n",
            "                                                  #\"background_noise\": {\"background_paths\": \"/mnt/home/rheinrich/deep_bird_detect/datapipeline/speech_command_dataset\" ,\"prob\":0.5,\n",
            "                                                  #                     \"min_snr_in_db\": 3.0, \"max_snr_in_db\": 30.0,},\n",
            "                                                  #\"pitch_shift\": {\"prob\": 0.5, \"min_transpose_semitones\": -4.0, \"max_transpose_semitones\": 4.0,},\n",
            "                                                 },\n",
            "                       \"spectrogram_augmentations\": {\"time_masking\": {\"time_mask_param\": 100, \"prob\": 0.5},\n",
            "                                                     \"frequency_masking\": {\"freq_mask_param\": 100, \"prob\": 0.5},\n",
            "                                                     \"time_stretch\": {\"prob\": 0.5, \"min_rate\": 0.8, \"max_rate\": 1.2}},\n",
            "                       \"learning_rate\": 0.01,\n",
            "                       \"batch_size\": 64,\n",
            "                       \"num_workers\": 32,\n",
            "                      }"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Create a HuggingFace dataset"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Here, we use the ESC50 dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_dataset, train_push_dataset, test_dataset, train_mean, train_std = create_dataset(path = \"ashraq/esc50\",\n",
            "                                                                                        split = \"train\",\n",
            "                                                                                        columns = [\"audio\", \"target\"],\n",
            "                                                                                        test_size = 0.2,\n",
            "                                                                                        use_spectrogram = config_datapipeline[\"use_spectrogram\"],\n",
            "                                                                                        waveform_augmentations = config_datapipeline[\"waveform_augmentations\"],\n",
            "                                                                                        spectrogram_augmentations =config_datapipeline[\"spectrogram_augmentations\"],\n",
            "                                                                                        n_fft = config_datapipeline[\"n_fft\"],\n",
            "                                                                                        hop_length = config_datapipeline[\"hop_length\"],\n",
            "                                                                                        n_mels = config_datapipeline[\"n_mels\"],\n",
            "                                                                                       )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Show transformations"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Input values & targets"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "fig, axs = plt.subplots(3,3, figsize=(20, 30))\n",
            "axs = axs.flatten()\n",
            "print(axs.shape)\n",
            "examples = train_dataset[:9]\n",
            "spectrograms = examples[\"input_values\"]\n",
            "labels = examples[\"target\"]\n",
            "for spectrogram, label, ax in zip(spectrograms, labels, axs):\n",
            "    ax.set_title(label)\n",
            "    im = ax.imshow(spectrogram.squeeze().numpy())\n",
            "    fig.colorbar(im, ax = ax)   "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Convert one spectrogram back to waveform audio data"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The GriffinLim transformation converts power spectrograms back to waveform."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "If Mel-scale spectrograms are used instead, the InverseMelScale transformation has to be applied first, see https://pytorch.org/audio/stable/transforms.html."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "n_fft = config_datapipeline[\"n_fft\"]\n",
            "hop_length=config_datapipeline[\"hop_length\"]\n",
            "#n_stft= n_fft // 2 + 1\n",
            "#n_mels=config_datapipeline[\"n_mels\"]\n",
            "#sample_rate=train_dataset[0][\"audio\"][\"sampling_rate\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The waveform files from InverseMelScale have very poor quality and long runtimes, so we should try to work with power spectrograms instead of mel spectrograms if possible!"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "#transform_to_waveform = torchvision.transforms.Compose([transforms.InverseMelScale(n_stft=n_stft, n_mels=n_mels, sample_rate=sample_rate, max_iter=10000), transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "transform_to_waveform = transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Select one sample from the training dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "example = train_dataset[103]\n",
            "example_spectrogram = example[\"input_values\"]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Convert the spectrogram to a waveform"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# undo z standardization\n",
            "example_spectrogram_denormalized = undo_standardize_tensor(x=example_spectrogram.unsqueeze(0), mean=train_mean, std=train_std)\n",
            "\n",
            "# squeeze batch dimension\n",
            "example_spectrogram_denormalized = example_spectrogram_denormalized.squeeze(0)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Note: since the spectrogram was log10 transformed in the AudioAugmentor, we also need to invert the log transformation."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# undo log10 transformation\n",
            "example_spectrogram_denormalized = torch.pow(10, example_spectrogram_denormalized)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "example_waveform = transform_to_waveform(example_spectrogram_denormalized)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Augmented spectrogram"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plt.imshow(example_spectrogram.squeeze().numpy())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### original waveform audio data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "original_waveform = example[\"audio\"][\"array\"]\n",
            "sample_rate = example[\"audio\"][\"sampling_rate\"]\n",
            "ipd.Audio(data=original_waveform, rate=sample_rate)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### the waveform obtained from the augmented spectrogram."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "ipd.Audio(data=example_waveform, rate=sample_rate)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Create a PyTorch dataloader from the HuggingFace dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_dataloader = DataLoader(train_dataset,\n",
            "                              batch_size=config_datapipeline[\"batch_size\"], \n",
            "                              num_workers=config_datapipeline[\"num_workers\"],\n",
            "                              shuffle=True, \n",
            "                              collate_fn = lambda x: collate_batch(x, return_category=False))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_dataloader = DataLoader(test_dataset,\n",
            "                             batch_size=config_datapipeline[\"batch_size\"],\n",
            "                             num_workers=config_datapipeline[\"num_workers\"],\n",
            "                             shuffle=False, \n",
            "                             collate_fn = lambda x: collate_batch(x, return_category=False))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Train a benchmark model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Create Lightning Data Module"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "class LitDataModule(pl.LightningDataModule):\n",
            "    def __init__(self,):\n",
            "        super().__init__()\n",
            "        \n",
            "    def setup(self, stage=None):\n",
            "        pass\n",
            "\n",
            "    def train_dataloader(self):\n",
            "        return train_dataloader\n",
            "    \n",
            "    def val_dataloader(self):\n",
            "        return test_dataloader"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Initiate Lightning Data Module"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "datamodule = LitDataModule()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Define benchmark model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Train Model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Callbacks"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
            "    monitor= 'val_loss',\n",
            "    dirpath='./saved_models/benchmark/efficientnet_b0/',\n",
            "    filename='best_efficientnet_b0_model',\n",
            "    save_top_k=1,\n",
            "    mode='min')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_loss',\n",
            "                                            patience = 15)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Initiate Model and Trainer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = LightningEfficientNet(baseline_architecture=\"efficientnet_b0\",\n",
            "                        num_classes=50, \n",
            "                        num_channels=1,\n",
            "                        learning_rate= 0.01,)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer = pl.Trainer(max_epochs= 100,\n",
            "                     accelerator='gpu',\n",
            "                     devices=1,\n",
            "                     callbacks=[checkpoint_callback, early_stopping, pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-2), RichProgressBar()],\n",
            "                     log_every_n_steps=5\n",
            "                    )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Start Training"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Fit model\n",
            "trainer.fit(model, datamodule = datamodule)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Load best Checkpoint"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = model.load_from_checkpoint('./saved_models/benchmark/efficientnet_b0/best_efficientnet_b0_model.ckpt', \n",
            "                                   baseline_architecture=\"efficientnet_b0\",\n",
            "                                   num_classes=50,\n",
            "                                   num_channels=1,\n",
            "                                   learning_rate= config_datapipeline[\"learning_rate\"])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Validate model"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Validation set"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "trainer.validate(model, dataloaders = datamodule)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "name": "python3",
         "language": "python",
         "display_name": "Python 3 (ipykernel)"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.10"
      }
   },
   "source": [
      "# Imports"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
      "vscode": {
         "languageId": "plaintext"
      }
   },
   "outputs": [],
   "source": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import torch\n",
      "\n",
      "from torch.utils.data import DataLoader\n",
      "\n",
      "from torchaudio import transforms\n",
      "\n",
      "import pytorch_lightning as pl\n",
      "from pytorch_lightning.callbacks import RichProgressBar\n",
      "\n",
      "import IPython.display as ipd"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### Augmentation module for our datapipeline"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "from experiments.models.benchmark_models import LightningEfficientNet\n",
      "from gadme.utils.preprocess import collate_batch, create_dataset, undo_standardize_tensor"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "# Define configuration file"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "config_datapipeline = {\"use_spectrogram\": True,\n",
      "                       \"n_fft\": 1024, # The height of the spectrogram will be n_fft // 2 + 1\n",
      "                       \"hop_length\": 512, # The number of frames in the spectrogram (i.e. it's width) will be (audio_length - n_fft) // hop_length + 1, where audio_length is the length of the audio signal.\n",
      "                       \"n_mels\": None,\n",
      "                       \"waveform_augmentations\": {\"colored_noise\": {\"prob\": 0.5, \"min_snr_in_db\": 3.0, \"max_snr_in_db\": 30.0,\n",
      "                                                                    \"min_f_decay\": -2.0, \"max_f_decay\": 2.0,},\n",
      "                                                  #\"background_noise\": {\"background_paths\": \"/mnt/home/rheinrich/deep_bird_detect/datapipeline/speech_command_dataset\" ,\"prob\":0.5,\n",
      "                                                  #                     \"min_snr_in_db\": 3.0, \"max_snr_in_db\": 30.0,},\n",
      "                                                  #\"pitch_shift\": {\"prob\": 0.5, \"min_transpose_semitones\": -4.0, \"max_transpose_semitones\": 4.0,},\n",
      "                                                 },\n",
      "                       \"spectrogram_augmentations\": {\"time_masking\": {\"time_mask_param\": 100, \"prob\": 0.5},\n",
      "                                                     \"frequency_masking\": {\"freq_mask_param\": 100, \"prob\": 0.5},\n",
      "                                                     \"time_stretch\": {\"prob\": 0.5, \"min_rate\": 0.8, \"max_rate\": 1.2}},\n",
      "                       \"learning_rate\": 0.01,\n",
      "                       \"batch_size\": 64,\n",
      "                       \"num_workers\": 32,\n",
      "                      }"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "# Create a HuggingFace dataset"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "### Here, we use the ESC50 dataset"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "train_dataset, train_push_dataset, test_dataset, train_mean, train_std = create_dataset(path = \"ashraq/esc50\",\n",
      "                                                                                        split = \"train\",\n",
      "                                                                                        columns = [\"audio\", \"target\"],\n",
      "                                                                                        test_size = 0.2,\n",
      "                                                                                        use_spectrogram = config_datapipeline[\"use_spectrogram\"],\n",
      "                                                                                        waveform_augmentations = config_datapipeline[\"waveform_augmentations\"],\n",
      "                                                                                        spectrogram_augmentations =config_datapipeline[\"spectrogram_augmentations\"],\n",
      "                                                                                        n_fft = config_datapipeline[\"n_fft\"],\n",
      "                                                                                        hop_length = config_datapipeline[\"hop_length\"],\n",
      "                                                                                        n_mels = config_datapipeline[\"n_mels\"],\n",
      "                                                                                       )"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "# Show transformations"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### Input values & targets"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "fig, axs = plt.subplots(3,3, figsize=(20, 30))\n",
      "axs = axs.flatten()\n",
      "print(axs.shape)\n",
      "examples = train_dataset[:9]\n",
      "spectrograms = examples[\"input_values\"]\n",
      "labels = examples[\"target\"]\n",
      "for spectrogram, label, ax in zip(spectrograms, labels, axs):\n",
      "    ax.set_title(label)\n",
      "    im = ax.imshow(spectrogram.squeeze().numpy())\n",
      "    fig.colorbar(im, ax = ax)   "
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### Convert one spectrogram back to waveform audio data"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "The GriffinLim transformation converts power spectrograms back to waveform."
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "If Mel-scale spectrograms are used instead, the InverseMelScale transformation has to be applied first, see https://pytorch.org/audio/stable/transforms.html."
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "n_fft = config_datapipeline[\"n_fft\"]\n",
      "hop_length=config_datapipeline[\"hop_length\"]\n",
      "#n_stft= n_fft // 2 + 1\n",
      "#n_mels=config_datapipeline[\"n_mels\"]\n",
      "#sample_rate=train_dataset[0][\"audio\"][\"sampling_rate\"]"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "The waveform files from InverseMelScale have very poor quality and long runtimes, so we should try to work with power spectrograms instead of mel spectrograms if possible!"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "#transform_to_waveform = torchvision.transforms.Compose([transforms.InverseMelScale(n_stft=n_stft, n_mels=n_mels, sample_rate=sample_rate, max_iter=10000), transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)])"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "transform_to_waveform = transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### Select one sample from the training dataset"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "example = train_dataset[103]\n",
      "example_spectrogram = example[\"input_values\"]"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### Convert the spectrogram to a waveform"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "# undo z standardization\n",
      "example_spectrogram_denormalized = undo_standardize_tensor(x=example_spectrogram.unsqueeze(0), mean=train_mean, std=train_std)\n",
      "\n",
      "# squeeze batch dimension\n",
      "example_spectrogram_denormalized = example_spectrogram_denormalized.squeeze(0)"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "Note: since the spectrogram was log10 transformed in the AudioAugmentor, we also need to invert the log transformation."
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "# undo log10 transformation\n",
      "example_spectrogram_denormalized = torch.pow(10, example_spectrogram_denormalized)"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "example_waveform = transform_to_waveform(example_spectrogram_denormalized)"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### Augmented spectrogram"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "plt.imshow(example_spectrogram.squeeze().numpy())"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### original waveform audio data"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "original_waveform = example[\"audio\"][\"array\"]\n",
      "sample_rate = example[\"audio\"][\"sampling_rate\"]\n",
      "ipd.Audio(data=original_waveform, rate=sample_rate)"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### the waveform obtained from the augmented spectrogram."
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "ipd.Audio(data=example_waveform, rate=sample_rate)"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "# Create a PyTorch dataloader from the HuggingFace dataset"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "train_dataloader = DataLoader(train_dataset,\n",
      "                              batch_size=config_datapipeline[\"batch_size\"], \n",
      "                              num_workers=config_datapipeline[\"num_workers\"],\n",
      "                              shuffle=True, \n",
      "                              collate_fn = lambda x: collate_batch(x, return_category=False))"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "test_dataloader = DataLoader(test_dataset,\n",
      "                             batch_size=config_datapipeline[\"batch_size\"],\n",
      "                             num_workers=config_datapipeline[\"num_workers\"],\n",
      "                             shuffle=False, \n",
      "                             collate_fn = lambda x: collate_batch(x, return_category=False))"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "# Train a benchmark model"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "### Create Lightning Data Module"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "class LitDataModule(pl.LightningDataModule):\n",
      "    def __init__(self,):\n",
      "        super().__init__()\n",
      "        \n",
      "    def setup(self, stage=None):\n",
      "        pass\n",
      "\n",
      "    def train_dataloader(self):\n",
      "        return train_dataloader\n",
      "    \n",
      "    def val_dataloader(self):\n",
      "        return test_dataloader"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "### Initiate Lightning Data Module"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "datamodule = LitDataModule()"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "## Define benchmark model"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "## Train Model"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "### Callbacks"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
      "    monitor= 'val_loss',\n",
      "    dirpath='./saved_models/benchmark/efficientnet_b0/',\n",
      "    filename='best_efficientnet_b0_model',\n",
      "    save_top_k=1,\n",
      "    mode='min')"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_loss',\n",
      "                                            patience = 15)"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "### Initiate Model and Trainer"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "model = LightningEfficientNet(baseline_architecture=\"efficientnet_b0\",\n",
      "                        num_classes=50, \n",
      "                        num_channels=1,\n",
      "                        learning_rate= 0.01,)"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "trainer = pl.Trainer(max_epochs= 100,\n",
      "                     accelerator='gpu',\n",
      "                     devices=1,\n",
      "                     callbacks=[checkpoint_callback, early_stopping, pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-2), RichProgressBar()],\n",
      "                     log_every_n_steps=5\n",
      "                    )"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "### Start Training"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "# Fit model\n",
      "trainer.fit(model, datamodule = datamodule)"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "### Load best Checkpoint"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "model = model.load_from_checkpoint('./saved_models/benchmark/efficientnet_b0/best_efficientnet_b0_model.ckpt', \n",
      "                                   baseline_architecture=\"efficientnet_b0\",\n",
      "                                   num_classes=50,\n",
      "                                   num_channels=1,\n",
      "                                   learning_rate= config_datapipeline[\"learning_rate\"])"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "### Validate model"
   ]
},
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
      "#### Validation set"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
      "trainer.validate(model, dataloaders = datamodule)"
   ]
},
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
}
],
"metadata": {
"kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
},
"language_info": {
   "codemirror_mode": {
      "name": "ipython",
      "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
}
},
"nbformat": 4,
"nbformat_minor": 4
}