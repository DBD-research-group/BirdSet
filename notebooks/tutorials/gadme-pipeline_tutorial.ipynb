{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed02a1b-72ee-475e-9794-a6c0396fea10",
   "metadata": {},
   "source": [
    "# GADME Data Pipeline Tutorial\n",
    "\n",
    "This Jupyter notebook provides a comprehensive guide to setting up and configuring a data pipeline tailored for bird classification in audio files. The tutorial is structured to walk you through each component of the pipeline, ensuring a clear understanding of its functionality and configuration. Whether you are processing raw audio data or spectrograms, this notebook aims to provide you with the necessary knowledge to efficiently set up your data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909c001-7d00-4a0f-a16c-c984e7d91740",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Prerequisites\n",
    "Before initiating the installation process of the GADME pipeline, it's crucial to ensure that your computing environment meets the following prerequisites:\n",
    "- **Python**: You should have Python version 3.10 or higher installed on your system.\n",
    "\n",
    "### Installation Steps\n",
    "The GADME pipeline can be installed using either of the two methods: via Conda with Pip, or using Poetry. Select the method that best suits your preference and follow the corresponding steps below.\n",
    "\n",
    "#### Using Conda and Pip\n",
    "\n",
    "1. **Create a Conda Environment**: Begin by setting up a dedicated environment for GADME. This is a best practice to manage dependencies and avoid potential conflicts with other packages in your system.\n",
    "\n",
    "   ```bash\n",
    "   conda create -n gadme python=3.10\n",
    "   ```\n",
    "\n",
    "   After the environment is successfully created, activate it:\n",
    "\n",
    "   ```bash\n",
    "   conda activate gadme\n",
    "   ```\n",
    "\n",
    "2. **Install GADME**: Proceed with cloning the GADME repository and installing the package in editable mode. This approach is beneficial as it allows any modifications you make to the GADME code to be reflected immediately without the need for reinstallation.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/DBD-research-group/GADME.git\n",
    "   cd GADME\n",
    "   pip install -e .\n",
    "   ```\n",
    "\n",
    "#### Using Poetry\n",
    "\n",
    "1. **Clone the Repository**: Start with cloning the GADME repository to your local machine and navigate to the cloned directory.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/DBD-research-group/GADME.git\n",
    "   cd GADME\n",
    "   ```\n",
    "\n",
    "2. **Configure Poetry**: Prepare the project for Poetry by renaming the `pyproject.poetry` file to `pyproject.toml`.\n",
    "\n",
    "   ```bash\n",
    "   mv pyproject.poetry pyproject.toml\n",
    "   ```\n",
    "\n",
    "3. **Install Dependencies and Activate Environment**: Install all the necessary dependencies with Poetry and then activate the Poetry shell environment.\n",
    "\n",
    "   ```bash\n",
    "   poetry install\n",
    "   poetry shell\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4f398-f18d-42e4-a6b5-02329f1f5567",
   "metadata": {},
   "source": [
    "## Log in to Huggingface\n",
    "\n",
    "Our datasets are shared via HuggingFace Datasets in our [HuggingFace GADME repository](https://huggingface.co/datasets/DBD-research-group/gadme_v1). Huggingface is a central hub for sharing and utilizing datasets and models, particularly beneficial for machine learning and data science projects. For accessing private datasets hosted on HuggingFace, you need to be authenticated. Here's how you can log in to HuggingFace:\n",
    "\n",
    "1. **Install HuggingFace CLI**: If you haven't already, you need to install the HuggingFace CLI (Command Line Interface). This tool enables you to interact with HuggingFace services directly from your terminal. You can install it using pip:\n",
    "\n",
    "   ```bash\n",
    "   pip install huggingface_hub\n",
    "   ```\n",
    "\n",
    "2. **Login via CLI**: Once the HuggingFace CLI is installed, you can log in to your HuggingFace account directly from your terminal. This step is essential for accessing private datasets or contributing to the HuggingFace community. Use the following command:\n",
    "\n",
    "   ```bash\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "\n",
    "   After executing this command, you'll be prompted to enter your HuggingFace credentials ([User Access Token](https://huggingface.co/docs/hub/security-tokens)). Once authenticated, your credentials will be saved locally, allowing seamless access to HuggingFace resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be30bd-772f-407f-9925-ef247dbc8219",
   "metadata": {},
   "source": [
    "## Configuration of GADME Data Pipeline\n",
    "\n",
    "The GADME Data Pipeline offers a robust and flexible configuration system, primarily designed to streamline the process of setting up your data processing environment. While this notebook presents hardcoded configurations for simplicity, it's important to note that these settings can be dynamically managed using advanced configuration tools like Hydra. Hydra is a powerful utility that enables flexible and scalable configuration management, allowing you to adapt the pipeline settings to various environments or use cases seamlessly. For an in-depth understanding of Hydra, consider visiting [Hydra's official documentation](https://hydra.cc/docs/intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a42c7-47a8-4fc5-aabc-94afaf5affce",
   "metadata": {},
   "source": [
    "### Necessary Imports\n",
    "\n",
    "Configuring the GADME Data Pipeline requires a set of specific Python modules and classes. Below is a list of essential imports, each playing a crucial role in the pipeline setup. Understanding the purpose and functionality of these components is key to effectively customizing the data pipeline according to your specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317e6835-2bf4-4dd7-88d0-74adfeee9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import torch_audiomentations\n",
    "import torchaudio\n",
    "import torchvision\n",
    "\n",
    "import src\n",
    "from src.datamodule.base_datamodule import DatasetConfig, LoaderConfig, LoadersConfig\n",
    "from src.datamodule.components.transforms import (\n",
    "    GADMETransformsWrapper,\n",
    "    PreprocessingConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee133f-d40b-49d6-8053-973f0f127090",
   "metadata": {},
   "source": [
    "In this section:\n",
    "- `hydra` is utilized for managing and orchestrating the configuration setup, enabling you to easily modify and extend your pipeline configuration.\n",
    "- `torch_audiomentations` provides a collection of audio augmentations, useful for enhancing your dataset and improving the robustness of your models.\n",
    "- `torchaudio` and `torchvision` are essential for processing audio and image data, respectively, offering a wide range of tools for data manipulation and transformation.\n",
    "- The `src` module contains the core functionalities of the GADME project, with specific classes like `DatasetConfig`, `LoaderConfig`, `LoadersConfig`, `GADMETransformsWrapper`, and `PreprocessingConfig` designed to configure different aspects of the data loading, preprocessing, and transformation pipeline.\n",
    "\n",
    "By carefully configuring these components, you can tailor the GADME Data Pipeline to your specific data processing and analysis needs, ensuring an efficient and effective workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158f248-890f-47d0-8b16-68758a314685",
   "metadata": {},
   "source": [
    "### 1. Dataset Configuration\n",
    "\n",
    "Configuring the dataset is a pivotal step in the GADME data pipeline, as it directly impacts how your data is handled, processed, and prepared for training. The `DatasetConfig` class provides a structured way to specify numerous parameters and settings, ensuring that the dataset aligns with the specific requirements of your project. Below is an illustration of how you might configure a dataset for use within the GADME pipeline, followed by a detailed explanation of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb31e37f-c07a-4781-b7fd-297fd41e50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"dataset_path\": \"../../data\",\n",
    "    \"dataset_name\": \"high_sierras\",\n",
    "    \"hf_path\": \"DBD-research-group/gadme_v1\",\n",
    "    \"hf_name\": \"high_sierras\",\n",
    "    \"rand_seed\": 42,\n",
    "    \"num_classes\": 22,\n",
    "    \"num_workers\": 1,\n",
    "    \"validation_size\": 0.2,\n",
    "    \"task\": \"multilabel\",\n",
    "    \"sample_rate\": 32000,\n",
    "    \"class_weights_loss\": None,\n",
    "    \"class_weights_sampler\": True,\n",
    "    \"class_limit\": 500,\n",
    "    \"\": 5,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cca9fe-2ac0-4418-93d1-e8ad5fd37786",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = DatasetConfig(\n",
    "        data_dir=config[\"dataset_path\"],\n",
    "        dataset_name=config[\"dataset_name\"],\n",
    "        hf_path=config[\"hf_path\"],\n",
    "        hf_name=config[\"hf_name\"],\n",
    "        seed=config[\"rand_seed\"],\n",
    "        n_classes=config[\"num_classes\"],\n",
    "        n_workers=config[\"num_workers\"],\n",
    "        val_split=config[\"validation_size\"],\n",
    "        task=config[\"task\"],\n",
    "        subset=None,\n",
    "        sampling_rate=config[\"sample_rate\"],\n",
    "        #class_weights_loss=config[\"class_weights_loss\"], #TODO: remove # after bug fix\n",
    "        #class_weights_sampler=config[\"class_weights_sampler\"], #TODO: remove # after bug fix\n",
    "        classlimit=config[\"class_limit\"],\n",
    "        eventlimit=config[\"\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea4694-e2e8-4b71-a2ad-6b004850f7e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here's a brief overview of the parameters used in the `DatasetConfig` class:\n",
    "\n",
    "- `data_dir`: Specifies the directory where the dataset files are stored. **Important**: The dataset uses a lot of disk space, so make sure you have enough storage available.\n",
    "- `dataset_name`: The name assigned to the dataset.\n",
    "- `hf_path`: The path to the dataset stored on HuggingFace.\n",
    "- `hf_name`: The name of the dataset on HuggingFace.\n",
    "- `seed`: A seed value for ensuring reproducibility across runs.\n",
    "- `n_classes`: The total number of distinct classes in the dataset.\n",
    "- `n_workers`: The number of worker processes used for data loading.\n",
    "- `val_split`: The proportion of the dataset reserved for validation.\n",
    "- `task`: Defines the type of task (e.g., 'multilabel' or 'multiclass').\n",
    "- `sampling_rate`: The sampling rate for audio data processing.\n",
    "- `class_weights_loss` (Deprecated): Previously used for applying class weights in loss calculation.\n",
    "- `class_weights_sampler`: Indicates whether to use class weights in the sampler for handling imbalanced datasets.\n",
    "- `class_limit`: The maximum number of samples per class.\n",
    "- ``: Defines the maximum number of audio events processed per audio file, capping the quantity to ensure balance across files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2a0b4-c98c-4e95-9b51-3e7f17d038eb",
   "metadata": {},
   "source": [
    "#### Important Note:\n",
    "- The `class_weights_loss` parameter is currently deprecated and only implemented for focal loss. It's recommended to utilize the `class_weights_sampler` instead, as it has shown to yield favorable results, particularly as evidenced by the winner of the [BirdCLEF 2023](https://www.kaggle.com/competitions/birdclef-2023) challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58a9d4-aabd-43b7-b6f6-e7acbb4cefeb",
   "metadata": {},
   "source": [
    "Selecting appropriate values for these parameters is crucial, as they directly influence the efficiency of the training process and the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734f65a-8fb3-4468-8fa2-7bccff9f8097",
   "metadata": {},
   "source": [
    "### 2. Dataloader Configuration\n",
    "\n",
    "Once the dataset is configured, the next crucial step is setting up the data loaders. Data loaders are pivotal in efficiently feeding data into the model during both the training and testing phases. They manage the data flow, ensuring that the model is supplied with a consistent stream of data batches. In this section, we'll use the `LoaderConfig` and `LoadersConfig` classes to define different configurations for the training and testing data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef515f7-b08d-4b7d-a95d-2fa9445b1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for batch sizes\n",
    "config[\"train_batch_size\"] = 32\n",
    "config[\"test_batch_size\"] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a49150e-4ce1-4ed4-abc7-9d376eb9c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the training data loader\n",
    "train_loader_config = LoaderConfig(\n",
    "    batch_size=config[\"train_batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=None,\n",
    ")\n",
    "\n",
    "# Configuration for the testing data loader\n",
    "test_loader_config = LoaderConfig(\n",
    "    batch_size=config[\"test_batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False,\n",
    "    #prefetch_factor=None,\n",
    ")\n",
    "\n",
    "# Aggregating the loader configurations\n",
    "loaders_config = LoadersConfig(\n",
    "    train=train_loader_config,\n",
    "    valid=test_loader_config,\n",
    "    test=test_loader_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42652-6857-4d0e-be29-8f50928aad0b",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `LoaderConfig` class:\n",
    "\n",
    "- `batch_size`: Specifies the number of samples contained in each batch. This is a crucial parameter as it impacts memory utilization and model performance.\n",
    "- `shuffle`: Determines whether the data is shuffled at the beginning of each epoch. Shuffling is typically used for training data to ensure model robustness and prevent overfitting.\n",
    "- `num_workers`: Sets the number of subprocesses to be used for data loading. More workers can speed up the data loading process but also increase memory consumption.\n",
    "- `pin_memory`: When set to `True`, enables the DataLoader to copy Tensors into CUDA pinned memory before returning them. This can lead to faster data transfer to CUDA-enabled GPUs.\n",
    "- `drop_last`: Determines whether to drop the last incomplete batch. Setting this to `True` is useful when the total size of the dataset is not divisible by the batch size.\n",
    "- `persistent_workers`: Indicates whether the data loader should keep the workers alive for the next epoch. This can improve performance at the cost of memory.\n",
    "- `prefetch_factor`: Defines the number of samples loaded in advance by each worker. This parameter is commented out here and can be adjusted based on specific requirements.\n",
    "\n",
    "Proper configuration of the data loaders is essential as it directly influences the efficiency of the training process, hardware resource utilization, and ultimately, the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a91ff-07b7-48f2-a115-7a15de770c52",
   "metadata": {},
   "source": [
    "### 3. Configuration of Data Preprocessing\n",
    "\n",
    "Data preprocessing is a fundamental step in the GADME data pipeline, ensuring that the raw data is adequately conditioned and transformed, making it suitable for model consumption. The `PreprocessingConfig` class allows for a detailed specification of various preprocessing parameters, each carefully selected to meet the unique demands of your dataset and model. Here's how you can configure the data preprocessing in the GADME pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2668b3d-8745-4a9e-a08c-bffe6622255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for preprocessing parameters\n",
    "config[\"n_fft\"] = 1024\n",
    "config[\"hop_length\"] = 320\n",
    "config[\"n_mels\"] = 128\n",
    "config[\"db_scale\"] = True\n",
    "config[\"power\"] = 2.0\n",
    "config[\"target_height\"] = None\n",
    "config[\"target_width\"] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "235ed383-b28f-4575-9a8b-02fff555e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Spectrogram\n",
    "from src.datamodule.components.resize import Resizer\n",
    "from src.datamodule.components.augmentations import PowerToDB\n",
    "\n",
    "# Creating the preprocessing configuration\n",
    "preprocessing_config = PreprocessingConfig(\n",
    "        spectrogram_conversion= Spectrogram(\n",
    "            n_fft=config[\"n_fft\"],\n",
    "            hop_length=config[\"hop_length\"],\n",
    "            power=config[\"power\"],\n",
    "        ),\n",
    "        resizer=Resizer(\n",
    "            db_scale=config[\"db_scale\"],\n",
    "            target_height=config[\"target_height\"],\n",
    "            target_width=config[\"target_width\"],\n",
    "        ),\n",
    "        dbscale_conversion=PowerToDB(),\n",
    "        normalize_spectrogram=True,\n",
    "        normalize_waveform=None,\n",
    "        mean=4.268, # calculated on AudioSet\n",
    "        std=4.569 # calculated on AudioSet\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff94aa-3191-41e8-a2ec-4393e8418c94",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `PreprocessingConfig` class:\n",
    "\n",
    "- `use_spectrogram`: Determines whether the audio data should be converted into a spectrogram, a visual representation of the spectrum of frequencies in the sound.\n",
    "- `n_fft`: The size of the FFT (Fast Fourier Transform) window, impacting the frequency resolution of the spectrogram.\n",
    "- `hop_length`: The number of samples between successive frames in the spectrogram. A smaller hop length leads to a higher time resolution.\n",
    "- `n_mels`: The number of Mel bands to generate. This parameter is crucial for the Mel spectrogram and impacts the spectral resolution.\n",
    "- `db_scale`: Indicates whether to scale the magnitude of the spectrogram to the decibel scale, which can help in visualizing the spectrum more clearly.\n",
    "- `target_height` and `target_width`: Specify the dimensions to which the spectrogram images will be resized. This can be important for maintaining consistency in input size for certain neural networks.\n",
    "- `normalize_spectrogram`: Whether to apply normalization to the spectrogram. Normalization can help in stabilizing the training process.\n",
    "- `normalize_waveform`: Determines whether to apply normalization to the raw waveform data.\n",
    "\n",
    "By accurately configuring these preprocessing parameters, you ensure that the input data to the model is standardized and optimized for the learning process, which is essential for achieving high performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2390d-937a-46c8-8829-94ff3eca2b28",
   "metadata": {},
   "source": [
    "### 4. Configuration of Transformations\n",
    "\n",
    "Transformations play a critical role in the data preparation process within the GADME data pipeline. These operations, applied before the data is fed into the model, encompass a range of augmentation techniques designed to regularize the model and prevent overfitting. Properly configured transformations not only enhance the diversity and quality of the training data but also help the model generalize better to new, unseen data.\n",
    "\n",
    "In the GADME framework, transformations are meticulously orchestrated through the `GADMETransformsWrapper` class. This wrapper acts as a comprehensive interface for defining and applying various transformations and augmentations to the data. It ensures that the data is consistently and effectively transformed, aligning with the specific requirements of the model and the inherent characteristics of the dataset.\n",
    "\n",
    "By configuring the `transforms_wrapper` using the `GADMETransformsWrapper` class, you gain precise control over how the data is manipulated during the preprocessing phase. This level of control is pivotal in tailoring the data pipeline to the nuances of your specific use case, ultimately leading to more robust and accurate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16af9b0-b4da-47a3-a551-433009c4739d",
   "metadata": {},
   "source": [
    "#### 4.1 Augmentations\n",
    "\n",
    "Augmentations are powerful techniques applied to the data to introduce diversity and variability. They are particularly useful in audio and signal processing to enhance the robustness of models against variations in input data. In the GADME framework, you can configure waveform and spectrogram augmentations as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306dfe3d-b6a1-41e9-ab21-ac551e501dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Waveform Augmentations**\n",
    "\n",
    "These augmentations are applied directly to the audio waveform. In GADME, you can use any waveform augmentation technique as long as it can be composed by the [torch-audiomentations Compose function](https://github.com/asteroid-team/torch-audiomentations/blob/main/torch_audiomentations/core/composition.py). You can add waveform augmentations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "263b67d3-0a2f-4ccf-b8eb-d55af0236ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"waveform_augmentations\"] = {\n",
    "    \"colored_noise\": {\n",
    "        \"_target_\": torch_audiomentations.AddColoredNoise,\n",
    "        \"p\": 0.2,\n",
    "        \"min_snr_in_db\": 3.0,\n",
    "        \"max_snr_in_db\": 30.0,\n",
    "        \"min_f_decay\": -2.0,\n",
    "        \"max_f_decay\": 2.0\n",
    "    },\n",
    "    \"pitch_shift\": {\n",
    "        \"_target_\": torch_audiomentations.PitchShift,\n",
    "        \"p\": 0.2,\n",
    "        \"sample_rate\": 32000,\n",
    "        \"min_transpose_semitones\": -4.0,\n",
    "        \"max_transpose_semitones\": 4.0,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c94eee-4cd7-49eb-b277-1902b48ced9e",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- `colored_noise`: Adds colored noise to the audio signal to simulate various real-world noise conditions.\n",
    "- `pitch_shift`: Alters the pitch of the audio signal, which is useful for simulating different tonal variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf77953-a58d-4e81-8e50-2d15be2ca0bf",
   "metadata": {},
   "source": [
    "**Spectrogram Augmentations**\n",
    "\n",
    "These augmentations are applied to the spectrogram representation of the audio. In GADME, you can use any spectrogram augmentation technique as long as it can be composed by the [torchvision Compose function](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html). You can add spectrogram augmentations as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58e9af4b-879b-4095-92da-99a5e0992c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"spectrogram_augmentations\"] = {\n",
    "    \"time_masking\": {\n",
    "        \"_target_\": torchvision.transforms.RandomApply,\n",
    "        \"p\": 0.3,\n",
    "        \"transforms\": \n",
    "        [{\n",
    "            \"_target_\": torchaudio.transforms.TimeMasking, # - _ --> list!\n",
    "            \"time_mask_param\": 100,\n",
    "            \"iid_masks\": True,\n",
    "        }],\n",
    "    },\n",
    "    \"frequency_masking\": {\n",
    "        \"_target_\": torchvision.transforms.RandomApply,\n",
    "        \"p\": 0.5,\n",
    "        \"transforms\":\n",
    "        [{\n",
    "            \"_target_\": torchaudio.transforms.FrequencyMasking, # - _ --> list!\n",
    "            \"freq_mask_param\": 100,\n",
    "            \"iid_masks\": True,\n",
    "        }]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aca7d4-609d-4519-8675-ee7a35fa52c2",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- `time_masking`: Randomly masks a sequence of consecutive time steps in the spectrogram, helping the model become invariant to small temporal shifts.\n",
    "- `frequency_masking`: Randomly masks a sequence of consecutive frequency channels, encouraging the model to be robust against frequency variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04268897-f6f4-4488-9d85-12cf23c15baf",
   "metadata": {},
   "source": [
    "Configuring the augmentations correctly is crucial as they directly influence the model's ability to learn from a diverse set of data representations, ultimately leading to better generalization and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee00cb-31db-4f66-883f-623791389e06",
   "metadata": {},
   "source": [
    "#### 4.2 Decoding\n",
    "\n",
    "Decoding is a process, that converts the (compressed) data into a format that can be directly used by the model. In the GADME framework, we use the `EventDecoding` class by default. It is designed for preprocessing audio files in the context of event detection tasks. Its primary function is to ensure that each audio segment fed into the model is not only in the correct format, but also conditioned to improve the model's ability to identify and understand different audio events. Decoding is configured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e2b022-ae03-4940-99de-3bf4823ce506",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"decoding\"] = {\n",
    "    \"_target_\": src.datamodule.components.EventDecoding,\n",
    "    \"min_len\": 1.0,\n",
    "    \"max_len\": 5.0,\n",
    "    \"sampling_rate\": 32000,\n",
    "    \"extension_time\": 8,\n",
    "    \"extracted_interval\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad60adc-3a7e-4506-9c7e-529074359b9f",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "- `_target_`: Specifies the EventDecoding component to be used in the data processing pipeline.\n",
    "- `min_len` and `max_len`: Determine the minimum and maximum duration (in seconds) of the audio segments after decoding. These constraints ensure that each processed audio segment is of a suitable length for the model.\n",
    "- `sampling_rate`: Defines the sampling rate to which the audio should be resampled. This standardizes the input data's sampling rate, making it consistent for model processing.\n",
    "- `extension_time`: Refers to the time (in seconds) by which the duration of an audio event is extended. This parameter is crucial for ensuring that shorter audio events are sufficiently long for the model to process effectively.\n",
    "- `extracted_interval`: Denotes the fixed duration (in seconds) of the audio segment that is randomly extracted from the extended audio event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6723e8-1b8c-4dcd-b55d-0e934f6f704f",
   "metadata": {},
   "source": [
    "Decoding is performed on the fly, ensuring that the data fed into the model is always in the correct format, even when the source data comes in various encoded forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af655211-4a1a-4015-b40f-a6f6fbf2647c",
   "metadata": {},
   "source": [
    "#### 4.3 Feature Extraction\n",
    "\n",
    "Feature extraction is a pivotal step in transforming raw data into a structured format that is suitable for model training. The `DefaultFeatureExtractor` in GADME is tailored for processing waveform data, providing a range of functionalities to prepare the data for model consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf56c70-dfc1-4bd4-8b91-a0e40ac06380",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"feature_extractor\"] = {\n",
    "    \"_target_\": src.datamodule.components.DefaultFeatureExtractor,\n",
    "    \"feature_size\": 1,\n",
    "    \"sampling_rate\": 32000,\n",
    "    \"padding_value\": 0.0,\n",
    "    \"return_attention_mask\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84eaf7-0d06-4a5d-91d1-55ede47349f5",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "- `_target_`: Specifies the feature extractor component used in the pipeline.\n",
    "- `feature_size`: Determines the size of the extracted features.\n",
    "- `sampling_rate`: The sampling rate at which the audio data should be processed.\n",
    "- `padding_value`: The value used for padding shorter sequences to a consistent length.\n",
    "- `return_attention_mask`: Indicates whether an attention mask should be returned along with the processed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae4db7-fa6a-4768-a00a-1e588d0534ac",
   "metadata": {},
   "source": [
    "This component is crucial for ensuring that the input data to the model is in a consistent and processable format, catering to models that require structured input in the form of PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde138d-2a3c-4644-aad8-10a37b585bd3",
   "metadata": {},
   "source": [
    "#### 4.4 No-call Sampler\n",
    "\n",
    "The no-call sampler is a sophisticated component in the GADME framework, designed to augment the dataset with 'no-call' samples. These samples are crucial for scenarios where the model needs to learn to recognize the absence of certain events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1610ad5-914e-4188-b36e-052d0777b93b",
   "metadata": {},
   "source": [
    "In the following configuration we don't use the nocall_sampler for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad57df45-2c7b-4da3-a8fe-fb04c7b44d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration without the no-call sampler\n",
    "config[\"nocall_sampler\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872a2dd-6990-4384-adf3-04b95eb58241",
   "metadata": {},
   "source": [
    "However, you can add a nocall_sampler as shown in the following code snippet. Here, `directory` is the path to the dataset you want to use to create no-call samples (for example, [DCASE18](https://dcase.community/challenge2018/index)).\n",
    "\n",
    "```python\n",
    "# Configuration to enable the no-call sampler\n",
    "config[\"nocall_sampler\"] = {\n",
    "    \"_target_\": src.datamodule.components.augmentations.NoCallMixer,\n",
    "    \"directory\": \"insert path here\",\n",
    "    \"p\": 0.075,\n",
    "    \"sampling_rate\": 32000,\n",
    "    \"length\": 5,\n",
    "    \"n_classes\": 22,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea693c-b484-4324-b7f1-dc409da32f52",
   "metadata": {},
   "source": [
    "Key Parameters:\n",
    "- `_target_`: Specifies the no-call sampler component in the pipeline.\n",
    "- `directory`: The directory containing the no-call data. It's essential to ensure that this path is correctly set to the location of your no-call samples.\n",
    "- `p`: The probability of a sample being replaced with a no-call sample. This parameter allows you to control the frequency of no-call samples in your dataset.\n",
    "- `sampling_rate`, `length`, and `n_classes`: These parameters should align with the rest of your dataset and model configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4449ea8-6498-4051-8d24-3a9677d98aa0",
   "metadata": {},
   "source": [
    "The no-call sampler effectively increases the diversity of the dataset by adding samples that represent the absence of specific events or classes. This helps in creating a more balanced dataset and enhancing the model's ability to generalize well to various input conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993142c-54a2-43a0-9e4e-b977571b03fd",
   "metadata": {},
   "source": [
    "#### 4.5 Combining all Transformations\n",
    "\n",
    "After configuring individual components like augmentations, decoding, and feature extraction, the next step in the GADME framework is to combine all these elements. This integration is facilitated by the `GADMETransformsWrapper` class, which serves as a interface for managing all data transformations. Here's how you can instantiate and integrate all transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca342571-637b-4add-9650-fba07fc53093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate waveform augmentations\n",
    "waveform_augmentations = {\n",
    "        name: hydra.utils.instantiate(aug)\n",
    "        for name, aug in config[\"waveform_augmentations\"].items()\n",
    "    }\n",
    "\n",
    "# Instantiate spectrogram augmentations\n",
    "spectrogram_augmentations = {\n",
    "    name: hydra.utils.instantiate(aug)\n",
    "    for name, aug in config[\"spectrogram_augmentations\"].items()\n",
    "}\n",
    "\n",
    "# Instantiate decoding\n",
    "decoding = hydra.utils.instantiate(config[\"decoding\"])\n",
    "\n",
    "# Instantiate feature extraction\n",
    "feature_extractor = hydra.utils.instantiate(config[\"feature_extractor\"])\n",
    "\n",
    "# Instantiate the no-call sampler\n",
    "nocall_sampler = hydra.utils.instantiate(config[\"nocall_sampler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e013db3-db0f-4a83-9379-ca277fd2d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['max_length'] = 5\n",
    "# Combine all components into the transforms wrapper\n",
    "transforms_wrapper = GADMETransformsWrapper(\n",
    "    task=config[\"task\"],\n",
    "    sampling_rate=config[\"sample_rate\"],\n",
    "    model_type=\"vision\",\n",
    "    preprocessing=preprocessing_config,\n",
    "    spectrogram_augmentations=spectrogram_augmentations,\n",
    "    waveform_augmentations=waveform_augmentations,\n",
    "    decoding=decoding,\n",
    "    feature_extractor=feature_extractor,\n",
    "    max_length=config[\"max_length\"],\n",
    "    n_classes=None,\n",
    "    nocall_sampler=nocall_sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de90efd-5373-43cb-a8b6-354d8c87686e",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `GADMETransformsWrapper` class:\n",
    "\n",
    "- `task`: Specifies the type of task (e.g., 'multiclass' or 'multilabel').\n",
    "- `sampling_rate`: The sampling rate at which the audio data should be processed.\n",
    "- `model_type`: Indicates the type of model (e.g. 'vision' for spectrogram-based models or 'waveform' for waveform-based models).\n",
    "- `preprocessing`: The preprocessing configuration defined earlier.\n",
    "- `spectrogram_augmentations` and `waveform_augmentations`: The sets of augmentations to be applied to the spectrogram and waveform data, respectively.\n",
    "- `decoding` and `feature_extractor`: Components responsible for data decoding and feature extraction.\n",
    "- `max_length`: The maximum length for the processed data segments in seconds.\n",
    "- `nocall_sampler`: The no-call sampler component, if configured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6c21c-e988-49c1-a8d0-c86f1bf494f9",
   "metadata": {},
   "source": [
    "#### Important Note:\n",
    "- The `n_classes` parameter is currently deprecated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909e404-ea94-4eaf-9e11-71fb0ac75f56",
   "metadata": {},
   "source": [
    "The proper configuration of these parameters is critical for aligning the transformations with the model's requirements and the characteristics of the dataset, thereby ensuring optimal data preparation and subsequent model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb170e58-deed-4b52-ba18-160681b5dba0",
   "metadata": {},
   "source": [
    "### 5. Configuration of Event Mappings\n",
    "\n",
    "Event mapping plays a pivotal role in the data pipeline, serving as the bridge between raw dataset events and the structured input required by the model. This process ensures that each event in the dataset is accurately represented and can be effectively utilized during model training. By default, we use [bambird](https://www.sciencedirect.com/science/article/pii/S1574954122004022?casa_token=HEbcdB5MyRMAAAAA:saYbr1WNlJTs-kAZOtzMrNt5r1sN_69E7bMjfCJu2A4zlLLFoIt-5-Cht2Wryg59851H_PWgfHzw) for event mapping, which is implemented in the `XCEventMapping` class. Within the GADME framework, event mappings are configured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "285e6db5-58ea-4e7b-9f04-1f38fee0639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for event mappings\n",
    "config[\"mapper\"] = {\n",
    "    \"biggest_cluster\": True,\n",
    "    \"\": None, # Redundant because of the  parameter in DatasetConfig\n",
    "    \"no_call\": False, # No-calls are already handled by the nocall_sampler\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4f30431-6972-4536-b323-88b7378e47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the event mapper\n",
    "mapper = src.datamodule.components.XCEventMapping(\n",
    "            biggest_cluster=config[\"mapper\"][\"biggest_cluster\"],\n",
    "            no_call=config[\"mapper\"][\"no_call\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd405878-31de-4fbc-9729-20410747c89d",
   "metadata": {},
   "source": [
    "Key Parameters in Event Mapping:\n",
    "- `biggest_cluster`: If set to `True`, the mapper focuses on the biggest cluster of events, which can be particularly useful for datasets with imbalanced event distributions.\n",
    "- ``: Specifies the maximum number of events to consider. This can be used to limit the scope of the mapping, although it's usually already managed by the `DatasetConfig`.\n",
    "- `no_call`: Indicates whether 'no-call' events should be included. In this configuration, it's set to `False` as the no-call samples are handled separately by the `nocall_sampler`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc44f5-852c-4231-bafd-4757ee27841f",
   "metadata": {},
   "source": [
    "Properly configuring the event mappings is essential for ensuring that the model receives accurately structured and meaningful data, which is a cornerstone for effective model training and robust performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704cfdb6-d25c-46b7-a1ee-47d4b834e01f",
   "metadata": {},
   "source": [
    "## Creating the GADME Datamodule\n",
    "\n",
    "The GADME Datamodule plays a central role in the GADME data pipeline, offering streamlined handling and preprocessing of GADME datasets to ensure they are primed for model training. Let's delve into the setup process:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60180aa6-285c-4879-abca-e88b29578897",
   "metadata": {},
   "source": [
    "### Imports\n",
    "First, we import the necessary modules. `GADMEDataModule` is responsible for managing the GADME datasets, while the `logging` module is used for logging information during the data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c76242c-928d-4404-922d-30c462681bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "import os\n",
    "\n",
    "from src.datamodule.gadme_datamodule import GADMEDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29444ab9-1b1c-4ea2-b85c-c55b280f2c16",
   "metadata": {},
   "source": [
    "### Creating Cache Directory\n",
    "The cache directory is a dedicated space for storing processed data. Utilizing a cache directory can significantly expedite subsequent data loading operations by avoiding redundant data processing. Here's how to create and manage a cache directory effectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c8b7b7a-2c44-40d1-a7db-1355d2aefe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the absolute path of the dataset\n",
    "logging.info(f\"Dataset path: <{os.path.abspath(config['dataset_path'])}>\")\n",
    "\n",
    "# Create the dataset directory if it does not exist\n",
    "os.makedirs(config[\"dataset_path\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82895c4e-93ac-4590-aeb0-b0acb1f9909b",
   "metadata": {},
   "source": [
    "This approach ensures:\n",
    "- Organized data management: By maintaining a structured directory for your datasets, you facilitate easier access and management of your data assets.\n",
    "- Efficient data loading: By caching processed data, subsequent loads are much faster, which is particularly beneficial when working with large datasets.\n",
    "\n",
    "By carefully setting up the GADME Datamodule and managing your cache directory, you enhance the efficiency and reliability of your data pipeline, ensuring that your datasets are always ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764504c-f167-40b4-9297-6ca1184c5668",
   "metadata": {},
   "source": [
    "### Datamodule Initialization\n",
    "\n",
    "The `GADMEDataModule` class plays a pivotal role in orchestrating the data pipeline. It consolidates the dataset configuration, data loaders, transformations, and event mappings into a cohesive structure, ensuring a clean and manageable workflow. Here's how the GADMEDataModule is initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0586ee6-9cad-4270-acb9-931cf046b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GADMEDataModule\n",
    "datamodule = GADMEDataModule(\n",
    "        dataset=dataset_config,\n",
    "        loaders=loaders_config,\n",
    "        transforms=transforms_wrapper,\n",
    "        mapper=mapper,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0982ec-199c-43bc-a2e6-8732f2356cc9",
   "metadata": {},
   "source": [
    "Here's a brief overview of the parameters used in the `GADMEDataModule` class:\n",
    "- `dataset`: The configuration settings for the dataset. It defines how the data is structured and managed.\n",
    "- `loaders`: Configuration settings for the data loaders, determining how data is batched and fed into the model.\n",
    "- `transforms`: The set of transformations and augmentations applied to the data, ensuring that it's properly conditioned for the model.\n",
    "- `mapper`: The event mapping configuration, essential for translating raw dataset events into a structured format that the model can interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab439e-2ee1-471f-9e38-15dfbc12cdfc",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The data preparation stage is where the actual data processing takes place. This stage is critical in ensuring that the data is correctly preprocessed, structured, and ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee400c19-41a3-4a5e-9011-509bdd319e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094824e73f7c4fb985b0eb278904b74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfd4dac54854556bd2924b834d510a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4dbe10b6974446882c942db07b61bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/125k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60383c7569884c109b885b7282ffac75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4168230336764341815e3e9602b732b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files #0:   0%|          | 0/2 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad82d7de5be74127b926934c8029d477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2cf6784cac4cb7b6a96df34c47ac65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files #1:   0%|          | 0/2 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed6e72d218c408d9fbbb5b356fcb093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/217M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f5b2e4294a4be294669718ea15b659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef968b2fc554f908cd12b4f4132d93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/96.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4f489a428046dfbf1a7b57be8ed5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/189k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bacc2070965445897f098f4d37c6e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454c3168c02548d391a4852e63d532c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f5c8c238be455788d30f54fdd80d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a632e515eae34743b811ad1a27a85df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a188e76b1d444c99e7c02fab2ada1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44733bf298e4ada9382b77b3fd33c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218a1871d95e4ce091f22f31cb051026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c04cb1eb044d2bb562873c6b74fb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a5f2e617f745e88563ee8e9853afb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cd3be4e0374e81b58ac8b73764630d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c9ae8d2aa547ecaa6902136b50cd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487ece09f9434179ab328a9b71822312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/218M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e1da82bac4489999916e1903e3cf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e3be2754474fde87bdae47d79f647b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ef55b3041c4a32ae59a72b0df71849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeefe15893a475089aef8bf4ae14834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/215M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38f481624c84c4cb6efc542dde0996f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9157a9aa14ce4f00a49fb69dca441b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/223M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7327eb00fb9c4650bad298da34a6fe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc487f5a278c48e1ab9f11086211165e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4c01c504994a9eaec41042608ec341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78256caf42ac4dce8dd93ebfd79f9cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed134e74a6f441f2bfe0c6f311c8eb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b910e62e404fa39603508f6a06e475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c40443c0934aaab48040e4565a4758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286ae314f0554e5cbf2cb70f0566970b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0553a4f3d8314272aec553870ea7f30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1918769d07294363a4892e5449badbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb838921ae0c4e6d8965a59f06956895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fa3e1a18284ad7882c3728d6e022b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f900202910434d2cb597f7ab0ca1e9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/210M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe2fc8399c24301bddedfbe1fbc60ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f294591f5b564e838f53ab0d29056513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316db5fe84dc45a0bd5838a1f3a7d66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/214M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9298ec8ddfc4147a187b49e826e570d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e93c20825de410883e3ef6fd919bd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6959be778743b0a26e73631b755cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #0:   0%|          | 0/2 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906fdf09a3e14a46b581f47152ed0fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #1:   0%|          | 0/2 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf1224875514e0fa946dc9c7aeb5b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e3ca2eb5fb4c70acb7dab7128017cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb1c63265c14e4abbf04d3a84dc3a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_5s split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c19bcc5327d4a3a908e98c20cda564d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8657e273d2450287c2232fad60d048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44643 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing labels: 100%|| 21/21 [00:02<00:00,  9.50it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9c85ce5cab422f9a296cec48c1bf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a3c96acaff4b7393a0fe56693355e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522a7009c49547fbb5a09e19ef0a842a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/17064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1e65a065b446819012060191044848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4266 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312ac8fd8a724939b902e35f3eab5fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9cf461-1fb8-4bb4-8db3-f7b4b4830036",
   "metadata": {},
   "source": [
    "The `prepare_data()` method encompasses various steps, including downloading the data (if not already locally available), applying the preprocessing steps defined in the transformations, and organizing the data into a format that is compatible with the model. It's a method that encapsulates the entire data preparation workflow, ensuring that the data is optimally prepared for the training process.\n",
    "\n",
    "This methodical approach to data preparation and modularization of the data pipeline components in the GADME framework contributes significantly to the efficiency, maintainability, and robustness of the machine learning lifecycle.\n",
    "\n",
    "**Hint**: If you recive an error concerning a not existing huggingface dataset, please make sure you are logged in to HuggingFace (see [Log in to Huggingface](#log-in-to-huggingface))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e3987-0f9a-4452-8b82-54c0ea96180e",
   "metadata": {},
   "source": [
    "### Datamodule Setup for Training Phase\n",
    "\n",
    "Setting up the datamodule for the training phase is a crucial step in the GADME data pipeline. This setup involves initializing the training and validation dataloaders, which play a vital role in supplying the model with data during the training process. The setup is performed using the `setup(stage=\"fit\")` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a252adb3-539e-434c-adcb-2ec2c1f7c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the datamodule for the training phase\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# Retrieve the training and validation dataloaders\n",
    "train_loader = datamodule.train_dataloader()\n",
    "validation_loader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2048c240-bac0-465e-836f-1d9718c3a1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_values', 'labels']),\n",
       " torch.Size([32, 1, 128, 1024]),\n",
       " torch.Size([32, 22]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch a sample batch from the training dataloader\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "# Inspect the keys and shapes of the data in the batch\n",
    "batch.keys(), batch[\"input_values\"].shape, batch[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00066181-de6f-4980-90dc-458d6b92f27d",
   "metadata": {},
   "source": [
    "This code snippet demonstrates:\n",
    "- The initialization of the training phase.\n",
    "- The retrieval of training and validation dataloaders.\n",
    "- Fetching and inspecting a sample batch from the training dataloader.\n",
    "- The shapes of `input_values` and `labels` indicate the batch size, number of channels (if applicable), and dimensions of the input data and labels, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed731d9c-7aea-49ac-9df9-4fc61774abbc",
   "metadata": {},
   "source": [
    "### Datamodule Setup for Test Phase\n",
    "\n",
    "Similarly, the datamodule is set up for the test phase to ensure that the model can be effectively evaluated using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5db95322-ca2b-4f8f-b419-40529d7c92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the datamodule for the test phase\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "# Retrieve the test dataloader\n",
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ed904-f95b-4bad-a03b-531de0a67dc7",
   "metadata": {},
   "source": [
    "The `setup(stage=\"test\")` method prepares the datamodule specifically for the test phase, and `test_dataloader()` retrieves the test dataloader, which is instrumental for batching and loading the test data efficiently during the model evaluation process.\n",
    "\n",
    "By methodically setting up the datamodule for both training and test phases, you ensure that the model has access to well-prepared data, which is essential for accurate training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567af44-494a-4547-b57b-9b487e57a520",
   "metadata": {},
   "source": [
    "### Usage in TensorFlow\n",
    "\n",
    "Utilizing the GADME datamodule in a TensorFlow environment involves integrating the prepared dataloaders with TensorFlow's training and evaluation workflows. This integration ensures that the data is fed into TensorFlow models efficiently and in a format that TensorFlow can process. Here's how you can set up the GADME datamodule for TensorFlow compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce1c99c0-201a-4cae-9e9e-d4c6f9512fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the datamodule for the training phase\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "# Retrieve the training and validation datasets\n",
    "train_loader = datamodule.train_dataset\n",
    "validation_loader = datamodule.val_dataset\n",
    "\n",
    "# Setup the datamodule for the test phase\n",
    "datamodule.setup(stage=\"test\")\n",
    "\n",
    "# Retrieve the test dataset\n",
    "test_loader = datamodule.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bda355-34f2-42b2-b57a-b6112f48f49c",
   "metadata": {},
   "source": [
    "#### Key Considerations:\n",
    "- `train_dataset`, `validation_dataset`, and `test_dataset` are the datasets prepared by the GADME datamodule, ready to be used in TensorFlow's training and evaluation routines.\n",
    "- It's important to ensure that these datasets are in a format compatible with TensorFlow. This might involve additional steps such as converting the data to `tf.data.Dataset` objects or applying necessary transformations to align with TensorFlow's data handling mechanisms.\n",
    "- More information on this integration process can be found in [HuggingFace's documentation](https://huggingface.co/docs/datasets/use_with_tensorflow).\n",
    "\n",
    "By following these steps, you can leverage the robust data preprocessing and management capabilities of the GADME datamodule within a TensorFlow environment, facilitating an efficient and streamlined model training and evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bf946-c60a-451b-b290-5249ac68e36b",
   "metadata": {},
   "source": [
    "### Mapping of Labels to eBird Codes\n",
    "\n",
    "The eBird codes in the GADME datasets are in integer format. However, we can map these numeric labels to their corresponding eBird codes as defined in the `dataset_info.json` file (it is created during data preprocessing in the folder where the preprocessed data is stored; i.e. in `data_dir` of the `DatasetConfig`). The `get_label_to_category_mapping_from_metadata` function does this by parsing the JSON file and creating a dictionary that maps each numeric label to its corresponding eBird code in string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ebe88e2-a74c-4dd5-94ab-9193418f764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def get_label_to_category_mapping_from_metadata(\n",
    "    file_path: str, task: str\n",
    ") -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Reads a JSON file and extracts the mapping of labels to eBird codes.\n",
    "\n",
    "    The function expects the JSON structure to be in a specific format, where the mapping\n",
    "    is a list of names located under the keys 'features' -> 'labels' -> 'names'.\n",
    "    The index in the list corresponds to the label, and the value at that index is the eBird code.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the JSON file containing the label to eBird code mapping.\n",
    "    - task (str): The type of task for which to get the mapping. Expected values are \"multiclass\" or \"multilabel\".\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, str]: A dictionary where each key is a label (integer) and the corresponding value is the eBird code.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the file at `file_path` does not exist.\n",
    "    - json.JSONDecodeError: If the file is not a valid JSON.\n",
    "    - KeyError: If the expected keys ('features', 'labels', 'names') are not found in the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the file and read the JSON data\n",
    "    with open(file_path, \"r\") as file:\n",
    "        dataset_info = json.load(file)\n",
    "\n",
    "    # Extract the list of eBird codes from the loaded JSON structure.\n",
    "    # Note: This assumes a specific structure of the JSON data.\n",
    "    # If the structure is different, this line will raise a KeyError.\n",
    "    if task == \"multiclass\":\n",
    "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"names\"]\n",
    "    elif task == \"multilabel\":\n",
    "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"feature\"][\"names\"]\n",
    "    else:\n",
    "        # If the task is not recognized (not multiclass or multilabel), raise an error.\n",
    "        raise NotImplementedError(\n",
    "            f\"Only the multiclass and multilabel tasks are implemented, not task {task}.\"\n",
    "        )\n",
    "\n",
    "    # Create a dictionary mapping each label (index) to the corresponding eBird code.\n",
    "    mapping = {label: ebird_code for label, ebird_code in enumerate(ebird_codes_list)}\n",
    "\n",
    "    return mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-xS3fZVNL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
