{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "5ed02a1b-72ee-475e-9794-a6c0396fea10",
            "metadata": {},
            "source": [
                "# BirdSet Data Pipeline Tutorial\n",
                "\n",
                "This Jupyter notebook provides a comprehensive guide to setting up and configuring a data pipeline tailored for bird classification in audio files. The tutorial is structured to walk you through each component of the pipeline, ensuring a clear understanding of its functionality and configuration. Whether you are processing raw audio data or spectrograms, this notebook aims to provide you with the necessary knowledge to efficiently set up your data pipeline."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0909c001-7d00-4a0f-a16c-c984e7d91740",
            "metadata": {},
            "source": [
                "## Installation\n",
                "\n",
                "### Prerequisites\n",
                "Before initiating the installation process of the BirdSet pipeline, it's crucial to ensure that your computing environment meets the following prerequisites:\n",
                "- **Python**: You should have Python version 3.10 or higher installed on your system.\n",
                "\n",
                "### Installation Steps\n",
                "The BirdSet pipeline can be installed using either of the two methods: via Conda with Pip, or using Poetry. Select the method that best suits your preference and follow the corresponding steps below.\n",
                "\n",
                "#### Using Conda and Pip\n",
                "\n",
                "1. **Create a Conda Environment**: Begin by setting up a dedicated environment for BirdSet. This is a best practice to manage dependencies and avoid potential conflicts with other packages in your system.\n",
                "\n",
                "   ```bash\n",
                "   conda create -n birdset python=3.10\n",
                "   ```\n",
                "\n",
                "   After the environment is successfully created, activate it:\n",
                "\n",
                "   ```bash\n",
                "   conda activate birdset\n",
                "   ```\n",
                "\n",
                "2. **Install BirdSet**: Proceed with cloning the BirdSet repository and installing the package in editable mode. This approach is beneficial as it allows any modifications you make to the BirdSet code to be reflected immediately without the need for reinstallation.\n",
                "\n",
                "   ```bash\n",
                "   git clone https://github.com/DBD-research-group/BirdSet.git\n",
                "   cd BirdSet\n",
                "   pip install -e .\n",
                "   ```\n",
                "\n",
                "#### Using Poetry\n",
                "\n",
                "1. **Clone the Repository**: Start with cloning the BirdSet repository to your local machine and navigate to the cloned directory.\n",
                "\n",
                "   ```bash\n",
                "   git clone https://github.com/DBD-research-group/BirdSet.git\n",
                "   cd BirdSet\n",
                "   ```\n",
                "\n",
                "2. **Install Dependencies and Activate Environment**: Install all the necessary dependencies with Poetry and then activate the Poetry shell environment.\n",
                "\n",
                "   ```bash\n",
                "   poetry install\n",
                "   poetry shell\n",
                "   ```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "41a4f398-f18d-42e4-a6b5-02329f1f5567",
            "metadata": {},
            "source": [
                "## Log in to Huggingface\n",
                "\n",
                "Our datasets are shared via HuggingFace Datasets in our [HuggingFace BirdSet repository](https://huggingface.co/datasets/DBD-research-group/birdset_v1). Huggingface is a central hub for sharing and utilizing datasets and models, particularly beneficial for machine learning and data science projects. For accessing private datasets hosted on HuggingFace, you need to be authenticated. Here's how you can log in to HuggingFace:\n",
                "\n",
                "1. **Install HuggingFace CLI**: If you haven't already, you need to install the HuggingFace CLI (Command Line Interface). This tool enables you to interact with HuggingFace services directly from your terminal. You can install it using pip:\n",
                "\n",
                "   ```bash\n",
                "   pip install huggingface_hub\n",
                "   ```\n",
                "\n",
                "2. **Login via CLI**: Once the HuggingFace CLI is installed, you can log in to your HuggingFace account directly from your terminal. This step is essential for accessing private datasets or contributing to the HuggingFace community. Use the following command:\n",
                "\n",
                "   ```bash\n",
                "   huggingface-cli login\n",
                "   ```\n",
                "\n",
                "   After executing this command, you'll be prompted to enter your HuggingFace credentials ([User Access Token](https://huggingface.co/docs/hub/security-tokens)). Once authenticated, your credentials will be saved locally, allowing seamless access to HuggingFace resources."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "89c29ea2",
            "metadata": {},
            "source": [
                "## TLDR;\n",
                "To get started with the default configuration, you can use the following code snippet to set up the BirdSet pipeline to load the [High Sierras](https://zenodo.org/records/7525805) test dataset (10,296 samples) including a train set (5,197 samples) with matching bird classes from [xeno-canto](https://xeno-canto.org/). The total size of the dataset is 6.2GB. The samples will be provided as spectrograms with a resolution of `128x1024` pixels in batches of size `32`, the labels are one-hot encoded for a multilabel classification task. Down below you find further information on how to configure the pipeline to your needs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9f345d07",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c090cb1239724482b8bfec6db9ad7884",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map (num_proc=3):   0%|          | 0/5460 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6b41467968d04ba4bff71b9b27863b9a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/38170 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing labels: 100%|██████████| 21/21 [00:01<00:00, 10.97it/s]\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0b6ee90f9415480899d9115e10947ff2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map (num_proc=3):   0%|          | 0/17940 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3773fb0fec15402f8a001c6ab1347971",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map (num_proc=3):   0%|          | 0/12000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d8c6e92906cf450aabb68a6a63673fd2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Saving the dataset (0/1 shards):   0%|          | 0/14352 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7420332f27144149be1a2ef5b07232cf",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Saving the dataset (0/1 shards):   0%|          | 0/3588 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "405ad50543e149a9aaab4d64d298d696",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([32, 1, 128, 1024])\n",
                        "torch.Size([32, 21])\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'input_values': tensor([[[[ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           ...,\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752]]],\n",
                            " \n",
                            " \n",
                            "         [[[ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           ...,\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752]]],\n",
                            " \n",
                            " \n",
                            "         [[[ 7.6032,  0.9214,  0.4223,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 6.4331, -0.2487, -0.7479,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [-3.6482, -3.6345, -1.7405,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           ...,\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752]]],\n",
                            " \n",
                            " \n",
                            "         ...,\n",
                            " \n",
                            " \n",
                            "         [[[ 3.0478,  2.0167,  2.4519,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 1.8776,  0.8465,  1.2817,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 4.7945,  0.3207,  2.3447,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           ...,\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752]]],\n",
                            " \n",
                            " \n",
                            "         [[[ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           ...,\n",
                            "           [ 4.7476,  4.4290,  4.5768,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 5.2763,  4.7171,  4.6113,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6112,  7.3805,  7.0380,  ..., 16.5752, 16.5752, 16.5752]]],\n",
                            " \n",
                            " \n",
                            "         [[[ 4.2814,  6.5422,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 3.1112,  5.3721,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 3.4126,  5.7364,  7.6776,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           ...,\n",
                            "           [ 5.5360,  5.9462,  5.5030,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 6.5742,  6.2841,  6.5375,  ..., 16.5752, 16.5752, 16.5752],\n",
                            "           [ 7.6776,  7.6776,  7.6776,  ..., 16.5752, 16.5752, 16.5752]]]]),\n",
                            " 'labels': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 1., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 1., 0.],\n",
                            "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 1.],\n",
                            "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 1., 0.],\n",
                            "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 1.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 1.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 1., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          1., 0., 0.],\n",
                            "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          1., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 0.],\n",
                            "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
                            "          0., 0., 1.]], dtype=torch.float16)}"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from birdset.datamodule.base_datamodule import DatasetConfig\n",
                "from birdset.datamodule.birdset_datamodule import BirdSetDataModule\n",
                "\n",
                "# initiate the data module\n",
                "dm = BirdSetDataModule(\n",
                "    dataset= DatasetConfig(\n",
                "        data_dir='../../data_birdset/HSN',\n",
                "        dataset_name='HSN',\n",
                "        hf_path='DBD-research-group/BirdSet',\n",
                "        hf_name='HSN',\n",
                "        n_classes=21,\n",
                "        n_workers=3,\n",
                "        val_split=0.2,\n",
                "        task=\"multilabel\",\n",
                "        classlimit=500,\n",
                "        eventlimit=5,\n",
                "        sampling_rate=32000,\n",
                "    ),\n",
                ")\n",
                "# prepare the data (download dataset, ...)\n",
                "dm.prepare_data()\n",
                "# setup the dataloaders\n",
                "dm.setup(stage=\"fit\")\n",
                "# get the dataloaders\n",
                "train_loader = dm.train_dataloader()\n",
                "# get the first batch\n",
                "batch = next(iter(train_loader))\n",
                "# get shape of the batch\n",
                "print(batch[\"input_values\"].shape)\n",
                "print(batch[\"labels\"].shape)\n",
                "batch\n",
                "   "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "5584fa7f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n"
                    ]
                }
            ],
            "source": [
                "from lightning import Trainer\n",
                "min_epochs = 1\n",
                "max_epochs = 5\n",
                "trainer = Trainer(min_epochs=min_epochs, max_epochs=max_epochs, accelerator=\"gpu\", devices=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "4481caa6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from birdset.modules.multilabel_module import MultilabelModule\n",
                "model = MultilabelModule(\n",
                "    len_trainset=dm.len_trainset,\n",
                "    task=dm.task,\n",
                "    batch_size=dm.train_batch_size,\n",
                "    num_epochs=max_epochs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "e6b4e077",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "BaseModule(\n",
                            "  (loss): BCEWithLogitsLoss()\n",
                            "  (model): EfficientNetClassifier(\n",
                            "    (model): EfficientNet(\n",
                            "      (features): Sequential(\n",
                            "        (0): Conv2dNormActivation(\n",
                            "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
                            "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (2): SiLU(inplace=True)\n",
                            "        )\n",
                            "        (1): Sequential(\n",
                            "          (0): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
                            "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (2): Conv2dNormActivation(\n",
                            "                (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
                            "          )\n",
                            "          (1): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
                            "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (2): Conv2dNormActivation(\n",
                            "                (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
                            "          )\n",
                            "        )\n",
                            "        (2): Sequential(\n",
                            "          (0): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
                            "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
                            "          )\n",
                            "          (1): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
                            "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
                            "          )\n",
                            "          (2): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
                            "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
                            "          )\n",
                            "        )\n",
                            "        (3): Sequential(\n",
                            "          (0): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
                            "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
                            "          )\n",
                            "          (1): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
                            "                (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
                            "          )\n",
                            "          (2): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
                            "                (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
                            "          )\n",
                            "        )\n",
                            "        (4): Sequential(\n",
                            "          (0): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
                            "                (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
                            "          )\n",
                            "          (1): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
                            "                (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
                            "          )\n",
                            "          (2): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
                            "                (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
                            "          )\n",
                            "          (3): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
                            "                (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
                            "          )\n",
                            "        )\n",
                            "        (5): Sequential(\n",
                            "          (0): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
                            "                (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
                            "          )\n",
                            "          (1): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
                            "                (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
                            "          )\n",
                            "          (2): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
                            "                (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
                            "          )\n",
                            "          (3): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
                            "                (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
                            "          )\n",
                            "        )\n",
                            "        (6): Sequential(\n",
                            "          (0): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
                            "                (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
                            "          )\n",
                            "          (1): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
                            "          )\n",
                            "          (2): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
                            "          )\n",
                            "          (3): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
                            "          )\n",
                            "          (4): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
                            "          )\n",
                            "        )\n",
                            "        (7): Sequential(\n",
                            "          (0): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
                            "                (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
                            "          )\n",
                            "          (1): MBConv(\n",
                            "            (block): Sequential(\n",
                            "              (0): Conv2dNormActivation(\n",
                            "                (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (1): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
                            "                (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "                (2): SiLU(inplace=True)\n",
                            "              )\n",
                            "              (2): SqueezeExcitation(\n",
                            "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "                (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
                            "                (activation): SiLU(inplace=True)\n",
                            "                (scale_activation): Sigmoid()\n",
                            "              )\n",
                            "              (3): Conv2dNormActivation(\n",
                            "                (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "                (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "              )\n",
                            "            )\n",
                            "            (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
                            "          )\n",
                            "        )\n",
                            "        (8): Conv2dNormActivation(\n",
                            "          (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
                            "          (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "          (2): SiLU(inplace=True)\n",
                            "        )\n",
                            "      )\n",
                            "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
                            "      (classifier): Sequential(\n",
                            "        (0): Dropout(p=0.2, inplace=True)\n",
                            "        (1): Linear(in_features=1280, out_features=21, bias=True)\n",
                            "      )\n",
                            "    )\n",
                            "  )\n",
                            "  (train_metric): cmAP()\n",
                            "  (valid_metric): cmAP()\n",
                            "  (test_metric): cmAP()\n",
                            "  (valid_metric_best): MaxMetric()\n",
                            "  (valid_add_metrics): MetricCollection(\n",
                            "    (MultilabelAUROC): MultilabelAUROC()\n",
                            "    (T1Accuracy): TopKAccuracy()\n",
                            "    (T3Accuracy): TopKAccuracy()\n",
                            "    (mAP): mAP(),\n",
                            "    prefix=val/\n",
                            "  )\n",
                            "  (test_add_metrics): MetricCollection(\n",
                            "    (MultilabelAUROC): MultilabelAUROC()\n",
                            "    (T1Accuracy): TopKAccuracy()\n",
                            "    (T3Accuracy): TopKAccuracy()\n",
                            "    (mAP): mAP(),\n",
                            "    prefix=test/\n",
                            "  )\n",
                            "  (test_complete_metrics): MetricCollection(\n",
                            "    (cmAP5): cmAP5(\n",
                            "      (multilabel_ap): MultilabelAveragePrecision()\n",
                            "    )\n",
                            "    (pcmAP): pcmAP(),\n",
                            "    prefix=test/\n",
                            "  )\n",
                            ")"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "93cc4646",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
                        "\n",
                        "  | Name                  | Type                   | Params\n",
                        "-----------------------------------------------------------------\n",
                        "0 | loss                  | BCEWithLogitsLoss      | 0     \n",
                        "1 | model                 | EfficientNetClassifier | 6.5 M \n",
                        "2 | train_metric          | cmAP                   | 0     \n",
                        "3 | valid_metric          | cmAP                   | 0     \n",
                        "4 | test_metric           | cmAP                   | 0     \n",
                        "5 | valid_metric_best     | MaxMetric              | 0     \n",
                        "6 | valid_add_metrics     | MetricCollection       | 0     \n",
                        "7 | test_add_metrics      | MetricCollection       | 0     \n",
                        "8 | test_complete_metrics | MetricCollection       | 0     \n",
                        "-----------------------------------------------------------------\n",
                        "6.5 M     Trainable params\n",
                        "0         Non-trainable params\n",
                        "6.5 M     Total params\n",
                        "26.158    Total estimated model params size (MB)\n"
                    ]
                }
            ],
            "source": [
                "trainer.fit(model, dm)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73be30bd-772f-407f-9925-ef247dbc8219",
            "metadata": {},
            "source": [
                "## Configuration of BirdSet Data Pipeline\n",
                "\n",
                "The BirdSet Data Pipeline offers a robust and flexible configuration system, primarily designed to streamline the process of setting up your data processing environment. While this notebook presents hardcoded configurations for simplicity, it's important to note that these settings can be dynamically managed using advanced configuration tools like Hydra. Hydra is a powerful utility that enables flexible and scalable configuration management, allowing you to adapt the pipeline settings to various environments or use cases seamlessly. For an in-depth understanding of Hydra, consider visiting [Hydra's official documentation](https://hydra.cc/docs/intro).\n",
                "\n",
                "Tipp! Detailed information is provided in the docstrings of the classes and functions. You can access them by hovering over the class or function name in your IDE or by opening the source file in a text editor."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7ad76228",
            "metadata": {},
            "source": [
                "### BirdSetDataModule\n",
                "The `BirdSetDataModule` is a [PyTorch Lightning DataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html#lightningdatamodule) that encapsulates the entire data pipeline. It inherits from `BaseDataModuleHF` which is a base class for all DataModules that use [HuggingFace datasets libary](https://huggingface.co/docs/datasets/index).\n",
                "\n",
                "To initialize the `BirdSetDataModule`, you need to provide the following parameters:\n",
                "\n",
                "```python\n",
                "from src.datamodule.birdset_datamodule import BirdSetDataModule\n",
                "\n",
                "data_module = BirdSetDataModule(\n",
                "    dataset=dataset_config, #dataset (DatasetConfig): The configuration for the dataset.\n",
                "    loaders=loaders_config, #loaders (LoadersConfig): The configuration for the loaders.\n",
                "    transforms=transforms, #transforms (BirdSetTransformsWrapper): The transforms to be applied to the data.\n",
                "    mapper=mapper #mapper (XCEventMapping): The mapping for the events.\n",
                ")\n",
                "```\n",
                "\n",
                "We will now walk through each of these parameters to understand their functionality and configuration.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5158f248-890f-47d0-8b16-68758a314685",
            "metadata": {},
            "source": [
                "### 1. Dataset Configuration\n",
                "\n",
                "Configuring the dataset is the first step in configuring the BirdSet data pipeline, here you specify which dataset you want to load, how many classes it has, and how the data is split into train, validation, and test sets. The `DatasetConfig` class is used to configure the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "33cca9fe-2ac0-4418-93d1-e8ad5fd37786",
            "metadata": {},
            "outputs": [],
            "source": [
                "from birdset.datamodule.base_datamodule import DatasetConfig\n",
                "\n",
                "dataset_config = DatasetConfig(\n",
                "    data_dir='../../data_birdset/HSN',\n",
                "    dataset_name='HSN',\n",
                "    hf_path='DBD-research-group/BirdSet',\n",
                "    hf_name='HSN',\n",
                "    n_classes=21,\n",
                "    n_workers=1,\n",
                "    val_split=0.2,\n",
                "    task=\"multilabel\",\n",
                "    subset=None,\n",
                "    sampling_rate=32000,\n",
                "    class_weights_sampler=None,\n",
                "    classlimit=500,\n",
                "    eventlimit=5,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e8ea4694-e2e8-4b71-a2ad-6b004850f7e1",
            "metadata": {
                "tags": []
            },
            "source": [
                "Here's a brief overview of the parameters used in the `DatasetConfig` class:\n",
                "\n",
                "- `data_dir`: Specifies the directory where the dataset files are stored. **Important**: The dataset uses a lot of disk space, so make sure you have enough storage available.\n",
                "- `dataset_name`: The name assigned to the dataset.\n",
                "- `hf_path`: The path to the dataset stored on HuggingFace.\n",
                "- `hf_name`: The name of the dataset on HuggingFace.\n",
                "- `seed`: A seed value for ensuring reproducibility across runs.\n",
                "- `n_classes`: The total number of distinct classes in the dataset.\n",
                "- `n_workers`: The number of worker processes used for data loading.\n",
                "- `val_split`: The proportion of the dataset reserved for validation.\n",
                "- `task`: Defines the type of task (e.g., 'multilabel' or 'multiclass').\n",
                "- `sampling_rate`: The sampling rate for audio data processing.\n",
                "- `class_weights_sampler`: Indicates whether to use class weights in the sampler for handling imbalanced datasets.\n",
                "- `class_limit`: The maximum number of samples per class.\n",
                "- `eventlimit`: Defines the maximum number of audio events processed per audio file, capping the quantity to ensure balance across files."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fdc2a0b4-c98c-4e95-9b51-3e7f17d038eb",
            "metadata": {},
            "source": [
                "#### Important Note:\n",
                "- The `class_weights_loss` parameter is currently deprecated and only implemented for focal loss. It's recommended to utilize the `class_weights_sampler` instead, as it has shown to yield favorable results, particularly as evidenced by the winner of the [BirdCLEF 2023](https://www.kaggle.com/competitions/birdclef-2023) challenge."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5f58a9d4-aabd-43b7-b6f6-e7acbb4cefeb",
            "metadata": {},
            "source": [
                "Selecting appropriate values for these parameters is crucial, as they directly influence the efficiency of the training process and the overall performance of the model."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5734f65a-8fb3-4468-8fa2-7bccff9f8097",
            "metadata": {},
            "source": [
                "### 2. Dataloader Configuration\n",
                "\n",
                "Once the dataset is configured, the next crucial step is setting up the data loaders. Data loaders are pivotal in efficiently feeding data into the model during both the training and testing phases. They manage the data flow, ensuring that the model is supplied with a consistent stream of data batches. In this section, we'll use the `LoaderConfig` and `LoadersConfig` classes to define different configurations for the training and testing data loaders."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "5a49150e-4ce1-4ed4-abc7-9d376eb9c3b4",
            "metadata": {},
            "outputs": [],
            "source": [
                "from birdset.datamodule.base_datamodule import LoaderConfig, LoadersConfig\n",
                "# Configuration for the training data loader\n",
                "train_loader_config = LoaderConfig(\n",
                "    batch_size=32,\n",
                "    shuffle=True,\n",
                "    num_workers=8,\n",
                "    pin_memory=False,\n",
                "    drop_last=True,\n",
                "    persistent_workers=False,\n",
                "    #prefetch_factor=None,\n",
                ")\n",
                "\n",
                "# Configuration for the testing data loader\n",
                "test_loader_config = LoaderConfig(\n",
                "    batch_size=32,\n",
                "    shuffle=False,\n",
                "    num_workers=8,\n",
                "    pin_memory=False,\n",
                "    drop_last=False,\n",
                "    persistent_workers=False,\n",
                "    #prefetch_factor=None,\n",
                ")\n",
                "\n",
                "# Aggregating the loader configurations\n",
                "loaders_config = LoadersConfig(\n",
                "    train=train_loader_config,\n",
                "    valid=test_loader_config,\n",
                "    test=test_loader_config,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ea42652-6857-4d0e-be29-8f50928aad0b",
            "metadata": {},
            "source": [
                "Here's a brief overview of the parameters used in the `LoaderConfig` class:\n",
                "\n",
                "- `batch_size`: Specifies the number of samples contained in each batch. This is a crucial parameter as it impacts memory utilization and model performance.\n",
                "- `shuffle`: Determines whether the data is shuffled at the beginning of each epoch. Shuffling is typically used for training data to ensure model robustness and prevent overfitting.\n",
                "- `num_workers`: Sets the number of subprocesses to be used for data loading. More workers can speed up the data loading process but also increase memory consumption.\n",
                "- `pin_memory`: When set to `True`, enables the DataLoader to copy Tensors into CUDA pinned memory before returning them. This can lead to faster data transfer to CUDA-enabled GPUs.\n",
                "- `drop_last`: Determines whether to drop the last incomplete batch. Setting this to `True` is useful when the total size of the dataset is not divisible by the batch size.\n",
                "- `persistent_workers`: Indicates whether the data loader should keep the workers alive for the next epoch. This can improve performance at the cost of memory.\n",
                "- `prefetch_factor`: Defines the number of samples loaded in advance by each worker. This parameter is commented out here and can be adjusted based on specific requirements.\n",
                "\n",
                "Proper configuration of the data loaders is essential as it directly influences the efficiency of the training process, hardware resource utilization, and ultimately, the performance of the model."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "09a2390d-937a-46c8-8829-94ff3eca2b28",
            "metadata": {},
            "source": [
                "### 3. Configuration of TransformationsWrapper\n",
                "\n",
                "Transformations play a critical role in the data preparation process within the BirdSet data pipeline. These operations, applied before the data is fed into the model, encompass a range of augmentation techniques designed to regularize the model and prevent overfitting. Properly configured transformations not only enhance the diversity and quality of the training data but also help the model generalize better to new, unseen data.\n",
                "\n",
                "In the BirdSet framework, transformations are meticulously orchestrated through the `BirdSetTransformsWrapper` class. This wrapper acts as a comprehensive interface for defining and applying various transformations and augmentations to the data. It ensures that the data is consistently and effectively transformed, aligning with the specific requirements of the model and the inherent characteristics of the dataset.\n",
                "\n",
                "By configuring the `transforms_wrapper` using the `BirdSetTransformsWrapper` class, you gain precise control over how the data is manipulated during the preprocessing phase.\n",
                "\n",
                "To initialize the `BirdSetTransformsWrapper`, you need to provide the following parameters:\n",
                "\n",
                "```python\n",
                "from src.datamodule.components.transforms import BirdSetTransformsWrapper\n",
                "\n",
                "transforms = BirdSetTransformsWrapper(\n",
                "    task: Literal['multiclass', 'multilabel'] = \"multilabel\",\n",
                "    sampling_rate: int = 32000,\n",
                "    model_type: Literal['vision', 'waveform'] = \"waveform\",\n",
                "    spectrogram_augmentations: DictConfig = DictConfig({}),\n",
                "    waveform_augmentations: DictConfig = DictConfig({}),\n",
                "    decoding: EventDecoding | None = None,\n",
                "    feature_extractor: DefaultFeatureExtractor = DefaultFeatureExtractor(),\n",
                "    max_length: int = 5,\n",
                "    nocall_sampler: DictConfig = DictConfig({}),\n",
                "    preprocessing: PreprocessingConfig = PreprocessingConfig()\n",
                ")\n",
                "```\n",
                "\n",
                "We will go through each of these parameters to understand their functionality and configuration."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "239b8432",
            "metadata": {},
            "source": [
                "#### 3.1 Dataset and Model specific parameters\n",
                "The following parameters ensure that the transformations are tailored to the specific requirements of the dataset and the model:\n",
                "\n",
                "- `task`: Defines the type of task (e.g., 'multilabel' or 'multiclass').\n",
                "- `sampling_rate`: The sampling rate for audio data processing.\n",
                "- `model_type`: Specifies the type of model (e.g., 'vision' or 'waveform'). In case of a vison model, the input data is expected to be a spectrogram, while for a waveform model, the input data is the raw audio waveform.\n",
                "- max_length: The maximum length of the audio files in seconds."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a16af9b0-b4da-47a3-a551-433009c4739d",
            "metadata": {},
            "source": [
                "#### 3.2 Augmentations\n",
                "\n",
                "Augmentations are powerful techniques applied to the data to introduce diversity and variability. They are particularly useful in audio and signal processing to enhance the robustness of models against variations in input data. In the BirdSet framework, you can configure waveform and spectrogram augmentations as follows:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "306dfe3d-b6a1-41e9-ab21-ac551e501dd9",
            "metadata": {
                "tags": []
            },
            "source": [
                "**Waveform Augmentations**\n",
                "\n",
                "These augmentations are applied directly to the audio waveform. In BirdSet, you can use any waveform augmentation technique as long as it can be composed by the [torch-audiomentations Compose function](https://github.com/asteroid-team/torch-audiomentations/blob/main/torch_audiomentations/core/composition.py). You can add waveform augmentations as follows:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "263b67d3-0a2f-4ccf-b8eb-d55af0236ac9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch_audiomentations import AddColoredNoise, PitchShift\n",
                "waveform_augmentation = {\n",
                "    \"colored_noise\": AddColoredNoise(p=0.2, min_snr_in_db=3.0, max_snr_in_db=30.0, min_f_decay=-2.0, max_f_decay=2.0),\n",
                "    \"pitch_shift\": PitchShift(p=0.2, sample_rate=32000, min_transpose_semitones=-4.0, max_transpose_semitones=4.0),\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "47c94eee-4cd7-49eb-b277-1902b48ced9e",
            "metadata": {},
            "source": [
                "In this example:\n",
                "- `colored_noise`: Adds colored noise to the audio signal to simulate various real-world noise conditions.\n",
                "- `pitch_shift`: Alters the pitch of the audio signal, which is useful for simulating different tonal variations."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "daf77953-a58d-4e81-8e50-2d15be2ca0bf",
            "metadata": {},
            "source": [
                "**Spectrogram Augmentations**\n",
                "\n",
                "These augmentations are applied to the spectrogram representation of the audio. In BirdSet, you can use any spectrogram augmentation technique as long as it can be composed by the [torchvision Compose function](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html). You can add spectrogram augmentations as follows:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "58e9af4b-879b-4095-92da-99a5e0992c9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchvision.transforms import RandomApply\n",
                "from torchaudio.transforms import TimeMasking, FrequencyMasking\n",
                "spectrogram_augmentations = {\n",
                "    \"time_masking\": RandomApply([TimeMasking(time_mask_param=100, iid_masks=True)], p=0.3),\n",
                "    \"frequency_masking\": RandomApply([FrequencyMasking(freq_mask_param=100, iid_masks=True)], p=0.5)\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e8aca7d4-609d-4519-8675-ee7a35fa52c2",
            "metadata": {},
            "source": [
                "In this example:\n",
                "- `time_masking`: Randomly masks a sequence of consecutive time steps in the spectrogram, helping the model become invariant to small temporal shifts.\n",
                "- `frequency_masking`: Randomly masks a sequence of consecutive frequency channels, encouraging the model to be robust against frequency variations."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "04268897-f6f4-4488-9d85-12cf23c15baf",
            "metadata": {},
            "source": [
                "Configuring the augmentations correctly is crucial as they directly influence the model's ability to learn from a diverse set of data representations, ultimately leading to better generalization and performance."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "08ee00cb-31db-4f66-883f-623791389e06",
            "metadata": {},
            "source": [
                "#### 3.3 Decoding\n",
                "\n",
                "Decoding is a process, that converts the (compressed) data into a format that can be directly used by the model. In the BirdSet framework, we use the `EventDecoding` class by default. It is designed for preprocessing audio files in the context of event detection tasks. Its primary function is to ensure that each audio segment fed into the model is not only in the correct format, but also conditioned to improve the model's ability to identify and understand different audio events. Decoding is configured as follows:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "b7e2b022-ae03-4940-99de-3bf4823ce506",
            "metadata": {},
            "outputs": [],
            "source": [
                "from birdset.datamodule.components import EventDecoding\n",
                "decoding = EventDecoding(\n",
                "    min_len=1.0,\n",
                "    max_len=5.0,\n",
                "    sampling_rate=32000,\n",
                "    extension_time=8,\n",
                "    extracted_interval=5,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8ad60adc-3a7e-4506-9c7e-529074359b9f",
            "metadata": {},
            "source": [
                "Key Parameters:\n",
                "- `_target_`: Specifies the EventDecoding component to be used in the data processing pipeline.\n",
                "- `min_len` and `max_len`: Determine the minimum and maximum duration (in seconds) of the audio segments after decoding. These constraints ensure that each processed audio segment is of a suitable length for the model.\n",
                "- `sampling_rate`: Defines the sampling rate to which the audio should be resampled. This standardizes the input data's sampling rate, making it consistent for model processing.\n",
                "- `extension_time`: Refers to the time (in seconds) by which the duration of an audio event is extended. This parameter is crucial for ensuring that shorter audio events are sufficiently long for the model to process effectively.\n",
                "- `extracted_interval`: Denotes the fixed duration (in seconds) of the audio segment that is randomly extracted from the extended audio event."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1f6723e8-1b8c-4dcd-b55d-0e934f6f704f",
            "metadata": {},
            "source": [
                "Decoding is performed on the fly, ensuring that the data fed into the model is always in the correct format, even when the source data comes in various encoded forms."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "af655211-4a1a-4015-b40f-a6f6fbf2647c",
            "metadata": {},
            "source": [
                "#### 3.4 Feature Extraction\n",
                "\n",
                "Feature extraction is a pivotal step in transforming raw data into a structured format that is suitable for model training. The `DefaultFeatureExtractor` in BirdSet is tailored for processing waveform data, providing a range of functionalities to prepare the data for model consumption."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "bbf56c70-dfc1-4bd4-8b91-a0e40ac06380",
            "metadata": {},
            "outputs": [],
            "source": [
                "from birdset.datamodule.components import DefaultFeatureExtractor\n",
                "feature_extractor = DefaultFeatureExtractor(\n",
                "    feature_size=1,\n",
                "    sampling_rate=32000,\n",
                "    padding_value=0.0,\n",
                "    return_attention_mask=False,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1f84eaf7-0d06-4a5d-91d1-55ede47349f5",
            "metadata": {},
            "source": [
                "Key Parameters:\n",
                "- `feature_size`: Determines the size of the extracted features.\n",
                "- `sampling_rate`: The sampling rate at which the audio data should be processed.\n",
                "- `padding_value`: The value used for padding shorter sequences to a consistent length.\n",
                "- `return_attention_mask`: Indicates whether an attention mask should be returned along with the processed features."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d6ae4db7-fa6a-4768-a00a-1e588d0534ac",
            "metadata": {},
            "source": [
                "This component is crucial for ensuring that the input data to the model is in a consistent and processable format, catering to models that require structured input in the form of PyTorch tensors."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98891b2a",
            "metadata": {},
            "source": [
                "#### 3.5 No-call Sampler\n",
                "You can use the `NoCallMixer` to add no-call samples to the dataset. This is particularly useful for training models to recognize the absence of bird calls. The `NoCallMixer` is configured as follows:\n",
                "```python\n",
                "from src.datamodule.components.no_call_sampler import NoCallSampler\n",
                "\n",
                "nocall_sampler = NoCallMixer(\n",
                "    directory: str = \"path/to/no_call_samples\",\n",
                "    p: float = 0.075,\n",
                "    sampling_rate: int = 32000,\n",
                "    length: int = 5,\n",
                "    n_classes: int = 21,\n",
                ")\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "2300978e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Since this would require to have the dataset downloaded (see `download_background_noise.ipynb`, we will not use this for now\n",
                "nocall_sampler = None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e75a91ff-07b7-48f2-a115-7a15de770c52",
            "metadata": {},
            "source": [
                "#### 3.6 Configuration of Data Preprocessing\n",
                "\n",
                "Data preprocessing is a fundamental step in the BirdSet data pipeline, ensuring that the raw data is adequately conditioned and transformed, making it suitable for model consumption. The `PreprocessingConfig` class allows for a detailed specification of various preprocessing parameters, each carefully selected to meet the unique demands of your dataset and model. Here's how you can configure the data preprocessing in the BirdSet pipeline:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "235ed383-b28f-4575-9a8b-02fff555e812",
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchaudio.transforms import Spectrogram\n",
                "from birdset.datamodule.components.resize import Resizer\n",
                "from birdset.datamodule.components.augmentations import PowerToDB\n",
                "from birdset.datamodule.components.transforms import PreprocessingConfig\n",
                "\n",
                "# Creating the preprocessing configuration\n",
                "preprocessing = PreprocessingConfig(\n",
                "        spectrogram_conversion= Spectrogram(\n",
                "            n_fft=1024,\n",
                "            hop_length=320,\n",
                "            power=2.0,\n",
                "        ),\n",
                "        resizer=Resizer(\n",
                "            db_scale=True,\n",
                "            target_height=None,\n",
                "            target_width=1024,\n",
                "        ),\n",
                "        dbscale_conversion=PowerToDB(),\n",
                "        normalize_spectrogram=True,\n",
                "        normalize_waveform=None,\n",
                "        mean=4.268, # calculated on AudioSet\n",
                "        std=4.569 # calculated on AudioSet\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0eff94aa-3191-41e8-a2ec-4393e8418c94",
            "metadata": {},
            "source": [
                "Here's a brief overview of the parameters used in the `PreprocessingConfig` class:\n",
                "\n",
                "- `spectrogram_conversion`: This is an instance of the Spectrogram class from torchaudio.transforms. It is used to convert the audio waveform to a spectrogram. The parameters n_fft, hop_length, and power are used to configure the spectrogram conversion.\n",
                "\n",
                "    - `n_fft`: Th` size of the FFT, which will also determine the size of the window used for the STFT. It is set to 1024.\n",
                "    - `hop_length`: The number of samples between successive frames in the STFT. It is set to 320.\n",
                "    - `power`: The exponent for the magnitude spectrogram, e.g., 1 for energy, 2 for power, etc. It is set to 2.0.\n",
                "    - `resizer`: This is an instance of the Resizer class from src.datamodule.components.resize. It is used to resize the spectrogram. The parameters db_scale and target_width are used to configure the resizing.\n",
                "\n",
                "- `db_scale`: If set to True, the spectrogram is converted to dB scale. It is set to True.\n",
                "- `target_height`: The target height for the resized spectrogram. It is not set in this case.\n",
                "- target_width: The target width for the resized spectrogram. It is set to 1024.\n",
                "- `dbscale_conversion`: This is an instance of the PowerToDB class from src.datamodule.components.augmentations. It is used to convert the spectrogram to a dB scale.\n",
                "\n",
                "- `normalize_spectrogram`: If set to True, the spectrogram is normalized. It is set to True.\n",
                "\n",
                "- `normalize_waveform`: If set to a value, the audio waveform is normalized. It is not set in this case.\n",
                "\n",
                "- ``mean``: The mean value used for normalization. It is set to 4.268, which is calculated on AudioSet.\n",
                "\n",
                "- `std`: The standard deviation used for normalization. It is set to 4.569, which is calculated on AudioSet.\n",
                "\n",
                "\n",
                "By accurately configuring these preprocessing parameters, you ensure that the input data to the model is standardized and optimized for the learning process, which is essential for achieving high performance."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d993142c-54a2-43a0-9e4e-b977571b03fd",
            "metadata": {},
            "source": [
                "#### 3.7 Initiating the BirdSetTransformsWrapper\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "267876cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "from birdset.datamodule.components.transforms import BirdSetTransformsWrapper\n",
                "transforms = BirdSetTransformsWrapper(\n",
                "    task=\"multilabel\",\n",
                "    sampling_rate=32000,\n",
                "    model_type=\"vision\",\n",
                "    spectrogram_augmentations=spectrogram_augmentations,\n",
                "    waveform_augmentations=waveform_augmentation,\n",
                "    decoding=decoding,\n",
                "    feature_extractor=feature_extractor,\n",
                "    max_length=5,\n",
                "    nocall_sampler=nocall_sampler,\n",
                "    preprocessing=preprocessing,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb170e58-deed-4b52-ba18-160681b5dba0",
            "metadata": {},
            "source": [
                "### 4. Configuration of Event Mappings\n",
                "\n",
                "Event mapping plays a pivotal role in the data pipeline, serving as the bridge between raw dataset events and the structured input required by the model. This process ensures that each event in the dataset is accurately represented and can be effectively utilized during model training. By default, we use [bambird](https://www.sciencedirect.com/science/article/pii/S1574954122004022?casa_token=HEbcdB5MyRMAAAAA:saYbr1WNlJTs-kAZOtzMrNt5r1sN_69E7bMjfCJu2A4zlLLFoIt-5-Cht2Wryg59851H_PWgfHzw) for event mapping, which is implemented in the `XCEventMapping` class. Within the BirdSet framework, event mappings are configured as follows:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "d4f30431-6972-4536-b323-88b7378e47c7",
            "metadata": {},
            "outputs": [],
            "source": [
                "from birdset.datamodule.components import XCEventMapping\n",
                "# Instantiate the event mapper\n",
                "mapper = XCEventMapping(\n",
                "            biggest_cluster=True,\n",
                "            no_call=False,\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bd405878-31de-4fbc-9729-20410747c89d",
            "metadata": {},
            "source": [
                "Key Parameters in Event Mapping:\n",
                "- `biggest_cluster`: If set to `True`, the mapper focuses on the biggest cluster of events, which can be particularly useful for datasets with imbalanced event distributions.\n",
                "- ``: Specifies the maximum number of events to consider. This can be used to limit the scope of the mapping, although it's usually already managed by the `DatasetConfig`.\n",
                "- `no_call`: Indicates whether 'no-call' events should be included. In this configuration, it's set to `False` as the no-call samples are handled separately by the `nocall_sampler`."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fedc44f5-852c-4231-bafd-4757ee27841f",
            "metadata": {},
            "source": [
                "Properly configuring the event mappings is essential for ensuring that the model receives accurately structured and meaningful data, which is a cornerstone for effective model training and robust performance."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "704cfdb6-d25c-46b7-a1ee-47d4b834e01f",
            "metadata": {},
            "source": [
                "## Creating the BirdSet Datamodule\n",
                "\n",
                "The BirdSet Datamodule plays a central role in the BirdSet data pipeline, offering streamlined handling and preprocessing of BirdSet datasets to ensure they are primed for model training. Let's delve into the setup process:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "60180aa6-285c-4879-abca-e88b29578897",
            "metadata": {},
            "source": [
                "### Imports\n",
                "First, we import the necessary modules. `BirdSetDataModule` is responsible for managing the BirdSet datasets, while the `logging` module is used for logging information during the data processing steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "0c76242c-928d-4404-922d-30c462681bb0",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging \n",
                "import os\n",
                "\n",
                "from birdset.datamodule.birdset_datamodule import BirdSetDataModule"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "29444ab9-1b1c-4ea2-b85c-c55b280f2c16",
            "metadata": {},
            "source": [
                "### Creating Cache Directory\n",
                "The cache directory is a dedicated space for storing processed data. Utilizing a cache directory can significantly expedite subsequent data loading operations by avoiding redundant data processing. Here's how to create and manage a cache directory effectively:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "8c8b7b7a-2c44-40d1-a7db-1355d2aefe82",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Log the absolute path of the dataset\n",
                "logging.info(f\"Dataset path: <{os.path.abspath(dataset_config.data_dir)}>\")\n",
                "\n",
                "# Create the dataset directory if it does not exist\n",
                "os.makedirs(dataset_config.data_dir, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "82895c4e-93ac-4590-aeb0-b0acb1f9909b",
            "metadata": {},
            "source": [
                "This approach ensures:\n",
                "- Organized data management: By maintaining a structured directory for your datasets, you facilitate easier access and management of your data assets.\n",
                "- Efficient data loading: By caching processed data, subsequent loads are much faster, which is particularly beneficial when working with large datasets.\n",
                "\n",
                "By carefully setting up the BirdSet Datamodule and managing your cache directory, you enhance the efficiency and reliability of your data pipeline, ensuring that your datasets are always ready for model training."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d764504c-f167-40b4-9297-6ca1184c5668",
            "metadata": {},
            "source": [
                "### Datamodule Initialization\n",
                "\n",
                "The `BirdSetDataModule` class plays a pivotal role in orchestrating the data pipeline. It consolidates the dataset configuration, data loaders, transformations, and event mappings into a cohesive structure, ensuring a clean and manageable workflow. Here's how the BirdSetDataModule is initialized:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "d0586ee6-9cad-4270-acb9-931cf046b6ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the BirdSetDataModule\n",
                "datamodule = BirdSetDataModule(\n",
                "        dataset=dataset_config,\n",
                "        loaders=loaders_config,\n",
                "        transforms=transforms,\n",
                "        mapper=mapper,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb0982ec-199c-43bc-a2e6-8732f2356cc9",
            "metadata": {},
            "source": [
                "Here's a brief overview of the parameters used in the `BirdSetDataModule` class:\n",
                "- `dataset`: The configuration settings for the dataset. It defines how the data is structured and managed.\n",
                "- `loaders`: Configuration settings for the data loaders, determining how data is batched and fed into the model.\n",
                "- `transforms`: The set of transformations and augmentations applied to the data, ensuring that it's properly conditioned for the model.\n",
                "- `mapper`: The event mapping configuration, essential for translating raw dataset events into a structured format that the model can interpret."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "88ab439e-2ee1-471f-9e38-15dfbc12cdfc",
            "metadata": {},
            "source": [
                "### Data Preparation\n",
                "\n",
                "The data preparation stage is where the actual data processing takes place. This stage is critical in ensuring that the data is correctly preprocessed, structured, and ready for model training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "ee400c19-41a3-4a5e-9011-509bdd319e7b",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "25ed54c457b9459fadb74493ed3d92ed",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/5197 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "329496e1a4724b6487a39ddb5017574b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/37176 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing labels: 100%|██████████| 21/21 [00:01<00:00, 11.28it/s]\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a86e43fb85444d11bb716594e9fc07ff",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/17348 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "56dbe5d77a9c4c5e8c9be8c7c0a76595",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "510f1066cf1e4fdc9bbd2ad8f26bca5b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Saving the dataset (0/1 shards):   0%|          | 0/13878 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d84626e477a2411a84f99d21b5a8f4ea",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Saving the dataset (0/1 shards):   0%|          | 0/3470 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8156322a98c544ea9ccedeb04814be28",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Prepare the data for training\n",
                "datamodule.prepare_data()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eb9cf461-1fb8-4bb4-8db3-f7b4b4830036",
            "metadata": {},
            "source": [
                "The `prepare_data()` method encompasses various steps, including downloading the data (if not already locally available), applying the preprocessing steps defined in the transformations, and organizing the data into a format that is compatible with the model. It's a method that encapsulates the entire data preparation workflow, ensuring that the data is optimally prepared for the training process.\n",
                "\n",
                "This methodical approach to data preparation and modularization of the data pipeline components in the BirdSet framework contributes significantly to the efficiency, maintainability, and robustness of the machine learning lifecycle.\n",
                "\n",
                "**Hint**: If you recive an error concerning a not existing huggingface dataset, please make sure you are logged in to HuggingFace (see [Log in to Huggingface](#log-in-to-huggingface))."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "932e3987-0f9a-4452-8b82-54c0ea96180e",
            "metadata": {},
            "source": [
                "### Datamodule Setup for Training Phase\n",
                "\n",
                "Setting up the datamodule for the training phase is a crucial step in the BirdSet data pipeline. This setup involves initializing the training and validation dataloaders, which play a vital role in supplying the model with data during the training process. The setup is performed using the `setup(stage=\"fit\")` method:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "a252adb3-539e-434c-adcb-2ec2c1f7c996",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup the datamodule for the training phase\n",
                "datamodule.setup(stage=\"fit\")\n",
                "\n",
                "# Retrieve the training and validation dataloaders\n",
                "train_loader = datamodule.train_dataloader()\n",
                "validation_loader = datamodule.val_dataloader()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "2048c240-bac0-465e-836f-1d9718c3a1ce",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(dict_keys(['input_values', 'labels']),\n",
                            " torch.Size([32, 1, 128, 1024]),\n",
                            " torch.Size([32, 21]))"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Fetch a sample batch from the training dataloader\n",
                "for batch in train_loader:\n",
                "    break\n",
                "\n",
                "# Inspect the keys and shapes of the data in the batch\n",
                "batch.keys(), batch[\"input_values\"].shape, batch[\"labels\"].shape"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00066181-de6f-4980-90dc-458d6b92f27d",
            "metadata": {},
            "source": [
                "This code snippet demonstrates:\n",
                "- The initialization of the training phase.\n",
                "- The retrieval of training and validation dataloaders.\n",
                "- Fetching and inspecting a sample batch from the training dataloader.\n",
                "- The shapes of `input_values` and `labels` indicate the batch size, number of channels (if applicable), and dimensions of the input data and labels, respectively."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ed731d9c-7aea-49ac-9df9-4fc61774abbc",
            "metadata": {},
            "source": [
                "### Datamodule Setup for Test Phase\n",
                "\n",
                "Similarly, the datamodule is set up for the test phase to ensure that the model can be effectively evaluated using the test data:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "5db95322-ca2b-4f8f-b419-40529d7c92b5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup the datamodule for the test phase\n",
                "datamodule.setup(stage=\"test\")\n",
                "\n",
                "# Retrieve the test dataloader\n",
                "test_loader = datamodule.test_dataloader()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7d5ed904-f95b-4bad-a03b-531de0a67dc7",
            "metadata": {},
            "source": [
                "The `setup(stage=\"test\")` method prepares the datamodule specifically for the test phase, and `test_dataloader()` retrieves the test dataloader, which is instrumental for batching and loading the test data efficiently during the model evaluation process.\n",
                "\n",
                "By methodically setting up the datamodule for both training and test phases, you ensure that the model has access to well-prepared data, which is essential for accurate training, validation, and testing."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2567af44-494a-4547-b57b-9b487e57a520",
            "metadata": {},
            "source": [
                "### Usage in TensorFlow\n",
                "\n",
                "Utilizing the BirdSet datamodule in a TensorFlow environment involves integrating the prepared dataloaders with TensorFlow's training and evaluation workflows. This integration ensures that the data is fed into TensorFlow models efficiently and in a format that TensorFlow can process. Here's how you can set up the BirdSet datamodule for TensorFlow compatibility:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "ce1c99c0-201a-4cae-9e9e-d4c6f9512fed",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup the datamodule for the training phase\n",
                "datamodule.setup(stage=\"fit\")\n",
                "\n",
                "# Retrieve the training and validation datasets\n",
                "train_loader = datamodule.train_dataset\n",
                "validation_loader = datamodule.val_dataset\n",
                "\n",
                "# Setup the datamodule for the test phase\n",
                "datamodule.setup(stage=\"test\")\n",
                "\n",
                "# Retrieve the test dataset\n",
                "test_loader = datamodule.test_dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d8bda355-34f2-42b2-b57a-b6112f48f49c",
            "metadata": {},
            "source": [
                "#### Key Considerations:\n",
                "- `train_dataset`, `validation_dataset`, and `test_dataset` are the datasets prepared by the BirdSet datamodule, ready to be used in TensorFlow's training and evaluation routines.\n",
                "- It's important to ensure that these datasets are in a format compatible with TensorFlow. This might involve additional steps such as converting the data to `tf.data.Dataset` objects or applying necessary transformations to align with TensorFlow's data handling mechanisms.\n",
                "- More information on this integration process can be found in [HuggingFace's documentation](https://huggingface.co/docs/datasets/use_with_tensorflow).\n",
                "\n",
                "By following these steps, you can leverage the robust data preprocessing and management capabilities of the BirdSet datamodule within a TensorFlow environment, facilitating an efficient and streamlined model training and evaluation process."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "456bf946-c60a-451b-b290-5249ac68e36b",
            "metadata": {},
            "source": [
                "### Mapping of Labels to eBird Codes\n",
                "\n",
                "The eBird codes in the BirdSet datasets are in integer format. However, we can map these numeric labels to their corresponding eBird codes as defined in the `dataset_info.json` file (it is created during data preprocessing in the folder where the preprocessed data is stored; i.e. in `data_dir` of the `DatasetConfig`). The `get_label_to_category_mapping_from_metadata` function does this by parsing the JSON file and creating a dictionary that maps each numeric label to its corresponding eBird code in string format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "9ebe88e2-a74c-4dd5-94ab-9193418f764b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Dict\n",
                "\n",
                "def get_label_to_category_mapping_from_metadata(\n",
                "    file_path: str, task: str\n",
                ") -> Dict[int, str]:\n",
                "    \"\"\"\n",
                "    Reads a JSON file and extracts the mapping of labels to eBird codes.\n",
                "\n",
                "    The function expects the JSON structure to be in a specific format, where the mapping\n",
                "    is a list of names located under the keys 'features' -> 'labels' -> 'names'.\n",
                "    The index in the list corresponds to the label, and the value at that index is the eBird code.\n",
                "\n",
                "    Args:\n",
                "    - file_path (str): The path to the JSON file containing the label to eBird code mapping.\n",
                "    - task (str): The type of task for which to get the mapping. Expected values are \"multiclass\" or \"multilabel\".\n",
                "\n",
                "    Returns:\n",
                "    - Dict[int, str]: A dictionary where each key is a label (integer) and the corresponding value is the eBird code.\n",
                "\n",
                "    Raises:\n",
                "    - FileNotFoundError: If the file at `file_path` does not exist.\n",
                "    - json.JSONDecodeError: If the file is not a valid JSON.\n",
                "    - KeyError: If the expected keys ('features', 'labels', 'names') are not found in the JSON structure.\n",
                "    \"\"\"\n",
                "\n",
                "    # Open the file and read the JSON data\n",
                "    with open(file_path, \"r\") as file:\n",
                "        dataset_info = json.load(file)\n",
                "\n",
                "    # Extract the list of eBird codes from the loaded JSON structure.\n",
                "    # Note: This assumes a specific structure of the JSON data.\n",
                "    # If the structure is different, this line will raise a KeyError.\n",
                "    if task == \"multiclass\":\n",
                "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"names\"]\n",
                "    elif task == \"multilabel\":\n",
                "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"feature\"][\"names\"]\n",
                "    else:\n",
                "        # If the task is not recognized (not multiclass or multilabel), raise an error.\n",
                "        raise NotImplementedError(\n",
                "            f\"Only the multiclass and multilabel tasks are implemented, not task {task}.\"\n",
                "        )\n",
                "\n",
                "    # Create a dictionary mapping each label (index) to the corresponding eBird code.\n",
                "    mapping = {label: ebird_code for label, ebird_code in enumerate(ebird_codes_list)}\n",
                "\n",
                "    return mapping"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b1993d85",
            "metadata": {},
            "source": [
                "**Train and evaluate a Model**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b82c7f68",
            "metadata": {},
            "source": [
                "**Step1:  Loading datamodule**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9297ebc8",
            "metadata": {},
            "outputs": [],
            "source": [
                "####################load and prepare dataset##################\n",
                "from birdset.datamodule.components.event_decoding import EventDecoding\n",
                "from birdset.datamodule.components.transforms import PreprocessingConfig, BirdSetTransformsWrapper\n",
                "from birdset.datamodule.base_datamodule import DatasetConfig\n",
                "from birdset.datamodule.base_datamodule import DatasetConfig\n",
                "from birdset.datamodule.birdset_datamodule import BirdSetDataModule\n",
                "\n",
                "transforms = BirdSetTransformsWrapper(model_type='vision',preprocessing=PreprocessingConfig(spectrogram_conversion= Spectrogram(\n",
                "            n_fft=1024,\n",
                "            hop_length=320,\n",
                "            power=2.0,\n",
                "        ),), decoding=EventDecoding(sampling_rate=32000), task=\"multilabel\")\n",
                "\n",
                "# initiate the data module\n",
                "dm = BirdSetDataModule(\n",
                "    dataset= DatasetConfig(\n",
                "        data_dir='/workspace/data_birdset/HSN',\n",
                "        dataset_name='HSN',\n",
                "        hf_path='DBD-research-group/BirdSet',\n",
                "        hf_name='HSN',\n",
                "        n_classes=21,\n",
                "        n_workers=3,\n",
                "        val_split=0.2,\n",
                "        task=\"multilabel\",\n",
                "        classlimit=500,\n",
                "        eventlimit=5,\n",
                "        sampling_rate=32000,\n",
                "    ),\n",
                "    transforms=transforms\n",
                ")\n",
                "# prepare the data (download dataset, ...)\n",
                "dm.prepare_data()\n",
                "# setup the dataloaders\n",
                "dm.setup(stage=\"fit\")\n",
                "# get the dataloaders\n",
                "train_loader = dm.train_dataloader()\n",
                "\n",
                "# get the first batch\n",
                "batch = next(iter(train_loader))\n",
                "# get shape of the batch\n",
                "print(batch[\"input_values\"].shape)\n",
                "print(batch[\"labels\"].shape)\n",
                "batch"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9f733052",
            "metadata": {},
            "source": [
                "**Step2: Training the Model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b50dc342",
            "metadata": {},
            "outputs": [],
            "source": [
                "#from train_loader = dm.train_dataloader() import Trainer\n",
                "from lightning import Trainer\n",
                "min_epochs = 1\n",
                "max_epochs = 5\n",
                "trainer = Trainer(min_epochs=min_epochs, max_epochs=max_epochs, accelerator=\"gpu\", devices=[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e9c893d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from birdset.modules.models.resnet import ResNetClassifier\n",
                "from birdset.modules.metrics.multilabel import MultilabelMetricsConfig\n",
                "from torch.nn import BCEWithLogitsLoss\n",
                "module = ResNetClassifier(\"resnet50\",21)\n",
                "#import torch\n",
                "#model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True),\n",
                "\n",
                "from birdset.modules.base_module import BaseModule,NetworkConfig\n",
                "NetworkConfig=NetworkConfig(\n",
                "        model=ResNetClassifier(baseline_architecture=\"resnet50\",num_classes =21),\n",
                "        model_name =\"resnet50\",\n",
                "        model_type=\"vision\",\n",
                "        torch_compile= False,\n",
                "        sample_rate=32000,\n",
                "        normalize_waveform=False,\n",
                "        normalize_spectrogram=True)\n",
                "\n",
                "\n",
                "model = BaseModule(\n",
                "    network=NetworkConfig,\n",
                "    loss=BCEWithLogitsLoss(),\n",
                "    metrics=MultilabelMetricsConfig(),\n",
                "    len_trainset=dm.len_trainset,\n",
                "    task=dm.task,\n",
                "    batch_size=dm.train_batch_size,\n",
                "    num_epochs=max_epochs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e9c88d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.fit(model, dm)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "425f7072",
            "metadata": {},
            "source": [
                "**Step3: Selecting a sample from the test dataset** \n",
                "\n",
                "in this step there are some additional codes which show how can you visualize your sample\n",
                "if you want to listen to the audio,see the waveform or the Spectogram of the audio, you need to follow the steps here.\n",
                "\n",
                "first load the test dataset by addressing it (\"DBD-research-group/BirdSet\") and specifying the name of dataset (\"HSN\") and selecting of split between (train/test/val).\n",
                "\n",
                "in this part by setting offset and duration you can just take a bird sound from an audio sample"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42637ac6",
            "metadata": {},
            "outputs": [],
            "source": [
                "## load the test dataset\n",
                "from datasets import load_dataset\n",
                "hsn_test = load_dataset(\"DBD-research-group/BirdSet\",\"HSN\", split=\"test\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfb28ca7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get a sampleaudio\n",
                "import librosa\n",
                "import torchaudio\n",
                "sample = 199\n",
                "sr = 32000\n",
                "# get sample audio\n",
                "sample_audio =  librosa.load(\n",
                "    hsn_test[sample]['filepath'],\n",
                "    sr=sr,\n",
                "    offset=hsn_test[sample]['start_time'],\n",
                "    duration=hsn_test[sample]['end_time'] - hsn_test[sample]['start_time'])\n",
                "sample_tensor = torchaudio.load(\n",
                "        hsn_test[sample]['filepath'],\n",
                "        normalize=True,\n",
                "        frame_offset=hsn_test[sample]['start_time'] * sr,\n",
                "        num_frames=(hsn_test[sample]['end_time'] - hsn_test[sample]['start_time']) * sr\n",
                "        )\n",
                "sample_audio = sample_audio[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "53409ee7",
            "metadata": {},
            "source": [
                "**Listen to the Audio**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4f5ad513",
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Audio\n",
                "# Play the audio\n",
                "# sr is sampling rate which is 32000 here\n",
                "Audio(data=sample_audio, rate=sr)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7718c537",
            "metadata": {},
            "source": [
                "**Plot the wave form of the Audio**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2bf8f9f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Calculate the time axis\n",
                "time = np.arange(len(sample_audio)) / sr\n",
                "# Plot the waveform\n",
                "plt.figure(figsize=(14, 5))\n",
                "plt.plot(time, sample_audio)\n",
                "plt.xlabel('Time (s)')\n",
                "plt.ylabel('Amplitude')\n",
                "plt.title('Waveform')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a1121b8",
            "metadata": {},
            "source": [
                "**Make an spectrogram form of the Audio**\n",
                "\n",
                "##power_to_db function changes the power to dB scale"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1f3bb357",
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchaudio.transforms import Spectrogram\n",
                "import librosa \n",
                "\n",
                "spectrogram_conversion= Spectrogram(n_fft=1024)\n",
                "spectrogram=spectrogram_conversion(sample_tensor[0])\n",
                "spectrogram_db = librosa.power_to_db(spectrogram.squeeze().numpy(), ref=np.max)\n",
                "\n",
                "# Plot the spectrogram\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.imshow(spectrogram_db, aspect='auto', origin='lower', extent=[0, len(sample_audio)/sr, 0, sr/2])\n",
                "plt.colorbar(format='%+2.0f dB')\n",
                "plt.title('Spectrogram')\n",
                "plt.xlabel('Time (s)')\n",
                "plt.ylabel('Frequency (Hz)')\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48682b5c",
            "metadata": {},
            "source": [
                "**Step4: Evaluate the Trained Model**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "46a21f88",
            "metadata": {},
            "source": [
                "**A) Evaluaion with whole test samples**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5c5e6b0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.test(model=model,dataloaders=test_loader,ckpt_path=\"best\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1715eda2",
            "metadata": {},
            "source": [
                "**B) Finding the eBird code for a test sample in multilabel Task** \n",
                "\n",
                "**B_1) Selecting a sample of test_dataloader**\n",
                "\n",
                "There is 2 way for selecting a sample from your test dataset, the first one is mentioned in step3 and take a row audio file and you need to change the format to Spectrogram and this is the second one which take a sample from dataloader which is already a Spectrogram."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "54687d50",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "# Load the trained model\n",
                "model.eval()  # Set the model to evaluation mode\n",
                "test_loader = dm.test_dataloader()\n",
                "batch2 = next(iter(test_loader))\n",
                "# Extract audio data from the batch\n",
                "audiox = batch2[\"input_values\"][0]  # Assuming the first sample in the batch\n",
                "label = batch2[\"labels\"][0]\n",
                "audiox=audiox.unsqueeze(0)\n",
                "print(\"Original audio shape:\", audiox.shape)\n",
                "print(\"Label shape:\", label.shape)\n",
                "# Pass the spectrogram data through the model for prediction\n",
                "with torch.no_grad():\n",
                "    output = model(audiox)\n",
                "print(output.shape,output)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8402b449",
            "metadata": {},
            "source": [
                "**B_2) Prediction of sample's labels**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0ac6b12",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interpret the model's prediction\n",
                "# This depends on how your model's prediction method is implemented\n",
                "# It could be returning class indices, probabilities, or even class labels directly\n",
                "import torch\n",
                "\n",
                "# Your tensor of logits\n",
                "logits_tensor = output\n",
                "\n",
                "# Apply sigmoid to convert logits to probabilities for each class\n",
                "probabilities = torch.sigmoid(logits_tensor)\n",
                "print(probabilities)\n",
                "# Define a threshold (e.g., 0.5) to determine positive labels\n",
                "threshold = 0.1\n",
                "\n",
                "# Get the predicted labels based on the threshold\n",
                "\n",
                "predicted_labels = (probabilities > threshold).nonzero().squeeze()\n",
                "\n",
                "# Print the predicted labels\n",
                "print(\"Predicted Labels:\", predicted_labels.tolist())\n",
                "label_indices = [label_set[1].item() for label_set in predicted_labels]\n",
                "\n",
                "print(\"label_indices\",label_indices)  "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ca33b626",
            "metadata": {},
            "source": [
                "**B_3) Mapping of labels to eBirdsCode**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80551a1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from typing import Dict\n",
                "\n",
                "def get_label_to_category_mapping_from_metadata(\n",
                "    file_path: str, task: str\n",
                ") -> Dict[int, str]:\n",
                "    \"\"\"\n",
                "    Reads a JSON file and extracts the mapping of labels to eBird codes.\n",
                "\n",
                "    The function expects the JSON structure to be in a specific format, where the mapping\n",
                "    is a list of names located under the keys 'features' -> 'labels' -> 'names'.\n",
                "    The index in the list corresponds to the label, and the value at that index is the eBird code.\n",
                "\n",
                "    Args:\n",
                "    - file_path (str): The path to the JSON file containing the label to eBird code mapping.\n",
                "    - task (str): The type of task for which to get the mapping. Expected values are \"multiclass\" or \"multilabel\".\n",
                "\n",
                "    Returns:\n",
                "    - Dict[int, str]: A dictionary where each key is a label (integer) and the corresponding value is the eBird code.\n",
                "\n",
                "    Raises:\n",
                "    - FileNotFoundError: If the file at `file_path` does not exist.\n",
                "    - json.JSONDecodeError: If the file is not a valid JSON.\n",
                "    - KeyError: If the expected keys ('features', 'labels', 'names') are not found in the JSON structure.\n",
                "    \"\"\"\n",
                "\n",
                "    # Open the file and read the JSON data\n",
                "    with open(file_path, \"r\") as file:\n",
                "        dataset_info = json.load(file)\n",
                "\n",
                "    # Extract the list of eBird codes from the loaded JSON structure.\n",
                "    # Note: This assumes a specific structure of the JSON data.\n",
                "    # If the structure is different, this line will raise a KeyError.\n",
                "    if task == \"multiclass\":\n",
                "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"names\"]\n",
                "    elif task == \"multilabel\":\n",
                "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"feature\"][\"names\"]\n",
                "    else:\n",
                "        # If the task is not recognized (not multiclass or multilabel), raise an error.\n",
                "        raise NotImplementedError(\n",
                "            f\"Only the multiclass and multilabel tasks are implemented, not task {task}.\"\n",
                "        )\n",
                "\n",
                "    # Create a dictionary mapping each label (index) to the corresponding eBird code.\n",
                "    mapping = {label: ebird_code for label, ebird_code in enumerate(ebird_codes_list)}\n",
                "\n",
                "    return mapping\n",
                "\n",
                "mapping = get_label_to_category_mapping_from_metadata(\n",
                "    file_path='/workspace/data_birdset/HSN/HSN_processed_42_467ad9795903cdde/train/dataset_info.json',\n",
                "    task='multilabel'\n",
                ")\n",
                "print(mapping)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b5d86f3f",
            "metadata": {},
            "source": [
                "**B_4) Convert Predection output to Class Label Name**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ca4a977a",
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted_class_names = [mapping[idx] for idx in label_indices]\n",
                "\n",
                "# Print the predicted class names\n",
                "print(\"Predicted Class Names:\", predicted_class_names)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "src-xS3fZVNL-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
