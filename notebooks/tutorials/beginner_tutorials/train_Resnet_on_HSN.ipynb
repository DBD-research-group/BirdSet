{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Multilabel Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this tutorial, we will train the ResNet on the HSN data which is a mutltilabel dataset and evaluate our model by the whole test data. Then we feed the model by an audio sample that we got from test_dataloader and will find the label of it. Finally, we show how you can visualize an audio sample but this time we get the audio sample from our test data set directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1) Import HSN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vscode/.cache/pypoetry/virtualenvs/birdset-xS3fZVNL-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2aeeb73939438892e3fa03ed50b3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/5460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed36fa983a648ffbabdbb8577912656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing labels: 100%|██████████| 21/21 [00:02<00:00, 10.25it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc724ba4cdd643a8bc28e982daec1e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/17940 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf38e07c3f24fc195d42ebe454fe9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4a34d620594b519a53113fa1e7217b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24957dc04dc842e0af93a7bb699c45a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb90e9fd3eb4b2b942ba4be99d036a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 128, 1024])\n",
      "torch.Size([32, 21])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[[[ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4477,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.2510,  5.9508,  5.1205,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           ...,\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752]]],\n",
       " \n",
       " \n",
       "         [[[ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           ...,\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752]]],\n",
       " \n",
       " \n",
       "         [[[-0.7895,  1.2041,  2.3324,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [-1.9596,  0.0339,  1.1622,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [-2.2366, -1.6199, -0.6905,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           ...,\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 5.4709,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 4.9894,  5.5589,  6.1095,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           ...,\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6284,  3.4346,  1.1936,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [-0.5417,  2.2644,  0.0235,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [-0.2225, -0.1224, -0.8859,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           ...,\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 6.4481,  6.4481,  6.4481,  ..., 16.5752, 16.5752, 16.5752]]],\n",
       " \n",
       " \n",
       "         [[[ 3.5110,  5.6785,  6.3093,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 2.3409,  4.5083,  5.1392,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 2.4154,  4.7622,  5.6915,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           ...,\n",
       "           [ 2.2082,  4.4429,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 2.2054,  4.4401,  6.4481,  ..., 16.5752, 16.5752, 16.5752],\n",
       "           [ 2.1914,  4.4261,  6.4481,  ..., 16.5752, 16.5752, 16.5752]]]]),\n",
       " 'labels': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.]], dtype=torch.float16)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from birdset.datamodule.base_datamodule import DatasetConfig\n",
    "from birdset.datamodule.birdset_datamodule import BirdSetDataModule\n",
    "from birdset.datamodule.components.event_decoding import EventDecoding\n",
    "from birdset.datamodule.components.transforms import PreprocessingConfig, BirdSetTransformsWrapper\n",
    "from torchaudio.transforms import Spectrogram\n",
    "\n",
    "transforms = BirdSetTransformsWrapper(model_type='vision',preprocessing=PreprocessingConfig(spectrogram_conversion= Spectrogram(\n",
    "            n_fft=1024,\n",
    "            hop_length=320,\n",
    "            power=2.0,\n",
    "        ),), decoding=EventDecoding(sampling_rate=32000), task=\"multilabel\")\n",
    "# initiate the data module\n",
    "dm = BirdSetDataModule(\n",
    "    dataset= DatasetConfig(\n",
    "        data_dir='../../data_birdset/HSN',\n",
    "        dataset_name='HSN',\n",
    "        hf_path='DBD-research-group/BirdSet',\n",
    "        hf_name='HSN',\n",
    "        n_classes=21,\n",
    "        n_workers=3,\n",
    "        val_split=0.2,\n",
    "        task=\"multilabel\",\n",
    "        classlimit=500,\n",
    "        eventlimit=5,\n",
    "        sampling_rate=32000,\n",
    "    ),\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2) Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data (download dataset, ...)\n",
    "dm.prepare_data()\n",
    "# setup the dataloaders\n",
    "dm.setup(stage=\"fit\")\n",
    "# get the dataloaders\n",
    "train_loader = dm.train_dataloader()\n",
    "test_loader = dm.test_dataloader()\n",
    "# get the first batch\n",
    "batch = next(iter(train_loader))\n",
    "# get shape of the batch\n",
    "print(batch[\"input_values\"].shape)\n",
    "print(batch[\"labels\"].shape)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3) Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Trainer \n",
    "\n",
    "min_epochs = 1\n",
    "max_epochs = 20\n",
    "trainer = Trainer(min_epochs=min_epochs, max_epochs=max_epochs, accelerator=\"gpu\", devices=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4) Prepare your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from birdset.modules.models.resnet import ResNetClassifier\n",
    "from birdset.modules.metrics.multilabel import MultilabelMetricsConfig\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "#module = ResNetClassifier(\"resnet50\",21)\n",
    "\n",
    "from birdset.modules.base_module import BaseModule,NetworkConfig\n",
    "NetworkConfig=NetworkConfig(\n",
    "        model=ResNetClassifier(baseline_architecture=\"resnet50\",num_classes =21),\n",
    "        model_name =\"resnet50\",\n",
    "        model_type=\"vision\",\n",
    "        torch_compile= False,\n",
    "        sample_rate=32000,\n",
    "        normalize_waveform=False,\n",
    "        normalize_spectrogram=True)\n",
    "\n",
    "\n",
    "model = BaseModule(\n",
    "    network=NetworkConfig,\n",
    "    loss=BCEWithLogitsLoss(),\n",
    "    metrics=MultilabelMetricsConfig(),\n",
    "    len_trainset=dm.len_trainset,\n",
    "    task=dm.task,\n",
    "    batch_size=dm.train_batch_size,\n",
    "    num_epochs=max_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModule(\n",
       "  (loss): BCEWithLogitsLoss()\n",
       "  (model): ResNetClassifier(\n",
       "    (model): ResNet(\n",
       "      (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=21, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_metric): cmAP()\n",
       "  (valid_metric): cmAP()\n",
       "  (test_metric): cmAP()\n",
       "  (valid_metric_best): MaxMetric()\n",
       "  (valid_add_metrics): MetricCollection(\n",
       "    (MultilabelAUROC): MultilabelAUROC()\n",
       "    (T1Accuracy): TopKAccuracy()\n",
       "    (T3Accuracy): TopKAccuracy()\n",
       "    (mAP): mAP(),\n",
       "    prefix=val/\n",
       "  )\n",
       "  (test_add_metrics): MetricCollection(\n",
       "    (MultilabelAUROC): MultilabelAUROC()\n",
       "    (T1Accuracy): TopKAccuracy()\n",
       "    (T3Accuracy): TopKAccuracy()\n",
       "    (mAP): mAP(),\n",
       "    prefix=test/\n",
       "  )\n",
       "  (test_complete_metrics): MetricCollection(\n",
       "    (cmAP5): cmAP5(\n",
       "      (multilabel_ap): MultilabelAveragePrecision()\n",
       "    )\n",
       "    (pcmAP): pcmAP(),\n",
       "    prefix=test/\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5) Initializing your model with HSN data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we train our model with 20 epochs to show how it works and make a visible pipline . For having reasonable results the model should be traind by more epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name                  | Type              | Params\n",
      "------------------------------------------------------------\n",
      "0 | loss                  | BCEWithLogitsLoss | 0     \n",
      "1 | model                 | ResNetClassifier  | 23.5 M\n",
      "2 | train_metric          | cmAP              | 0     \n",
      "3 | valid_metric          | cmAP              | 0     \n",
      "4 | test_metric           | cmAP              | 0     \n",
      "5 | valid_metric_best     | MaxMetric         | 0     \n",
      "6 | valid_add_metrics     | MetricCollection  | 0     \n",
      "7 | test_add_metrics      | MetricCollection  | 0     \n",
      "8 | test_complete_metrics | MetricCollection  | 0     \n",
      "------------------------------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.179    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e84ecedf1ca43b99c8f8dd43042c92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2563a0ddbf81439e8bbbf512ca5e67b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6149b1dfdfc640c399c23496ed6d5457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424caa084e0c4c82861ef246c4773b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b7742e3cf04c4891c31c413944b786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3e23c43e594c88a9fa3586d7013753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1c754df1df432d8eee4d9371c1459e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6) Mapping of labels to eBirdsCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "def get_label_to_category_mapping_from_metadata(\n",
    "    file_path: str, task: str\n",
    ") -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Reads a JSON file and extracts the mapping of labels to eBird codes.\n",
    "\n",
    "    The function expects the JSON structure to be in a specific format, where the mapping\n",
    "    is a list of names located under the keys 'features' -> 'labels' -> 'names'.\n",
    "    The index in the list corresponds to the label, and the value at that index is the eBird code.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the JSON file containing the label to eBird code mapping.\n",
    "    - task (str): The type of task for which to get the mapping. Expected values are \"multiclass\" or \"multilabel\".\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, str]: A dictionary where each key is a label (integer) and the corresponding value is the eBird code.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the file at `file_path` does not exist.\n",
    "    - json.JSONDecodeError: If the file is not a valid JSON.\n",
    "    - KeyError: If the expected keys ('features', 'labels', 'names') are not found in the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the file and read the JSON data\n",
    "    with open(file_path, \"r\") as file:\n",
    "        dataset_info = json.load(file)\n",
    "\n",
    "    # Extract the list of eBird codes from the loaded JSON structure.\n",
    "    # Note: This assumes a specific structure of the JSON data.\n",
    "    # If the structure is different, this line will raise a KeyError.\n",
    "    if task == \"multiclass\":\n",
    "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"names\"]\n",
    "    elif task == \"multilabel\":\n",
    "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"feature\"][\"names\"]\n",
    "    else:\n",
    "        # If the task is not recognized (not multiclass or multilabel), raise an error.\n",
    "        raise NotImplementedError(\n",
    "            f\"Only the multiclass and multilabel tasks are implemented, not task {task}.\"\n",
    "        )\n",
    "\n",
    "    # Create a dictionary mapping each label (index) to the corresponding eBird code.\n",
    "    mapping = {label: ebird_code for label, ebird_code in enumerate(ebird_codes_list)}\n",
    "\n",
    "    return mapping\n",
    "\n",
    "mapping = get_label_to_category_mapping_from_metadata(\n",
    "    file_path='../../data_birdset/HSN/HSN_processed_42_467ad9795903cdde/train/dataset_info.json',\n",
    "    task='multilabel'\n",
    ")\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step7) Evaluate your model with your test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model,dataloaders=test_loader,ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step8) Finding ebirdcode of one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load the trained model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loader = dm.test_dataloader()\n",
    "batch2 = next(iter(test_loader))\n",
    "# Extract audio data from the batch\n",
    "audiox = batch2[\"input_values\"][0]  # Assuming the first sample in the batch\n",
    "label = batch2[\"labels\"][0]\n",
    "audiox=audiox.unsqueeze(0)\n",
    "print(\"Original audio shape:\", audiox.shape)\n",
    "print(\"Label shape:\", label.shape)\n",
    "\n",
    "# Pass the spectrogram data through the model for prediction\n",
    "with torch.no_grad():\n",
    "    output = model(audiox)\n",
    "print(output.shape,output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step9) Prediction of sample's labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the model's prediction\n",
    "# This depends on how your model's prediction method is implemented\n",
    "# It could be returning class indices, probabilities, or even class labels directly\n",
    "import torch\n",
    "\n",
    "# Your tensor of logits\n",
    "logits_tensor = output\n",
    "\n",
    "# Apply sigmoid to convert logits to probabilities for each class\n",
    "probabilities = torch.sigmoid(logits_tensor)\n",
    "print(probabilities)\n",
    "# Define a threshold (e.g., 0.5) to determine positive labels\n",
    "threshold = 0.1\n",
    "\n",
    "# Get the predicted labels based on the threshold\n",
    "\n",
    "predicted_labels = (probabilities > threshold).nonzero().squeeze()\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", predicted_labels.tolist())\n",
    "label_indices = [label_set[1].item() for label_set in predicted_labels]\n",
    "\n",
    "print(\"label_indices\",label_indices)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step10)Convert Predection output to Class Label Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_names = [mapping[idx] for idx in label_indices]\n",
    "real_label=[mapping[idx] for idx in label]\n",
    "print(\"Real Class Names\",real_label)\n",
    "# Print the predicted class names\n",
    "print(\"Predicted Class Names:\", predicted_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visaulization of Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "hsn_test = load_dataset(\"DBD-research-group/BirdSet\",\"HSN\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torchaudio\n",
    "sample = 199\n",
    "sr = 32000\n",
    "# get sample audio\n",
    "sample_audio =  librosa.load(\n",
    "    hsn_test[sample]['filepath'],\n",
    "    sr=sr,\n",
    "    offset=hsn_test[sample]['start_time'],\n",
    "    duration=hsn_test[sample]['end_time'] - hsn_test[sample]['start_time'])\n",
    "sample_tensor = torchaudio.load(\n",
    "        hsn_test[sample]['filepath'],\n",
    "        normalize=True,\n",
    "        frame_offset=hsn_test[sample]['start_time'] * sr,\n",
    "        num_frames=(hsn_test[sample]['end_time'] - hsn_test[sample]['start_time']) * sr\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load one bird sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio = sample_audio[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "# Play the audio\n",
    "Audio(data=sample_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'waveform' is a dictionary containing the audio path under the key 'path'\n",
    "# audio_path = waveform[\"path\"]\n",
    "\n",
    "# Load the audio file\n",
    "# audio, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Calculate the time axis\n",
    "time = np.arange(len(sample_audio)) / sr\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(time, sample_audio)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Waveform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Spectrogram\n",
    "import librosa \n",
    "\n",
    "spectrogram_conversion= Spectrogram(n_fft=1024)\n",
    "spectrogram=spectrogram_conversion(sample_tensor[0])\n",
    "spectrogram_db = librosa.power_to_db(spectrogram.squeeze().numpy(), ref=np.max)\n",
    "# Plot the spectrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(spectrogram_db, aspect='auto', origin='lower', extent=[0, len(sample_audio)/sr, 0, sr/2])\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdset-xS3fZVNL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
