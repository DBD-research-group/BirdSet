{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing_Evaluating_Visualizing a Model\n",
    "\n",
    "> In this tutorial we will learn how to Initializing, Evaluating a model and Visualizing an audio sample\n",
    "\n",
    "> We try to Initializing ResNet on HSN data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################load and prepare dataset##################  \n",
    "from birdset.datamodule.components.event_decoding import EventDecoding\n",
    "from birdset.datamodule.components.transforms import PreprocessingConfig, BirdSetTransformsWrapper\n",
    "from birdset.datamodule.base_datamodule import DatasetConfig\n",
    "from birdset.datamodule.base_datamodule import DatasetConfig\n",
    "from birdset.datamodule.birdset_datamodule import BirdSetDataModule\n",
    "from torchaudio.transforms import Spectrogram\n",
    "\n",
    "transforms = BirdSetTransformsWrapper(model_type='vision',preprocessing=PreprocessingConfig(spectrogram_conversion= Spectrogram(\n",
    "            n_fft=1024,\n",
    "            hop_length=320,\n",
    "            power=2.0,\n",
    "        ),), decoding=EventDecoding(sampling_rate=32000), task=\"multilabel\")\n",
    "\n",
    "# initiate the data module\n",
    "dm = BirdSetDataModule(\n",
    "    dataset= DatasetConfig(\n",
    "        data_dir='../../data_birdset/HSN',\n",
    "        dataset_name='HSN',\n",
    "        hf_path='DBD-research-group/BirdSet',\n",
    "        hf_name='HSN',\n",
    "        n_classes=21,\n",
    "        n_workers=3,\n",
    "        val_split=0.2,\n",
    "        task=\"multilabel\",\n",
    "        classlimit=500,\n",
    "        eventlimit=5,\n",
    "        sampling_rate=32000,\n",
    "    ),\n",
    "    transforms=transforms\n",
    ")\n",
    "# prepare the data (download dataset, ...)\n",
    "dm.prepare_data()\n",
    "# setup the dataloaders\n",
    "dm.setup(stage=\"fit\")\n",
    "# get the dataloaders\n",
    "train_loader = dm.train_dataloader()\n",
    "test_loader = dm.test_dataloader()\n",
    "# get the first batch\n",
    "batch = next(iter(train_loader))\n",
    "# get shape of the batch\n",
    "print(batch[\"input_values\"].shape)\n",
    "print(batch[\"labels\"].shape)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: Prepare Trainer\n",
    "\n",
    "> In this tutorial we train our model with 20 epochs to show how it works and make a visible pipeline. For having reasonable results the model should be trained by more epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from train_loader = dm.train_dataloader() import Trainer\n",
    "from lightning import Trainer\n",
    "min_epochs = 1\n",
    "max_epochs = 20\n",
    "trainer = Trainer(min_epochs=min_epochs, max_epochs=max_epochs, accelerator=\"gpu\", devices=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdset.modules.models.resnet import ResNetClassifier\n",
    "from birdset.modules.metrics.multilabel import MultilabelMetricsConfig\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "module = ResNetClassifier(\"resnet50\",21)\n",
    "#import torch\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True),\n",
    "\n",
    "from birdset.modules.base_module import BaseModule,NetworkConfig\n",
    "NetworkConfig=NetworkConfig(\n",
    "        model=ResNetClassifier(baseline_architecture=\"resnet50\",num_classes =21),\n",
    "        model_name =\"resnet50\",\n",
    "        model_type=\"vision\",\n",
    "        torch_compile= False,\n",
    "        sample_rate=32000,\n",
    "        normalize_waveform=False,\n",
    "        normalize_spectrogram=True)\n",
    "\n",
    "\n",
    "model = BaseModule(\n",
    "    network=NetworkConfig,\n",
    "    loss=BCEWithLogitsLoss(),\n",
    "    metrics=MultilabelMetricsConfig(),\n",
    "    len_trainset=dm.len_trainset,\n",
    "    task=dm.task,\n",
    "    batch_size=dm.train_batch_size,\n",
    "    num_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Initializing your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Selecting a single sample from the test dataset\n",
    "\n",
    "> In this step and the next one, there are some additional codes which show how you can visualize your sample\n",
    "\n",
    "> First load the test dataset by addressing it (\"DBD-research-group/BirdSet\") and specifying the name of dataset (\"HSN\") and selecting the split between (train/test/val).\n",
    "In this part by setting offset and duration you can just take a bird sound from an audio sample.\n",
    "\n",
    "> If you want to listen to the audio, see the waveform or the spectrogram of the audio, you need to follow steps in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the test dataset\n",
    "from datasets import load_dataset\n",
    "hsn_test = load_dataset(\"DBD-research-group/BirdSet\",\"HSN\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an audio sample\n",
    "import librosa\n",
    "import torchaudio\n",
    "sample = 199\n",
    "sr = 32000\n",
    " \n",
    "sample_audio =  librosa.load(\n",
    "    hsn_test[sample]['filepath'],\n",
    "    sr=sr,\n",
    "    offset=hsn_test[sample]['start_time'],\n",
    "    duration=hsn_test[sample]['end_time'] - hsn_test[sample]['start_time'])\n",
    "sample_tensor = torchaudio.load(\n",
    "        hsn_test[sample]['filepath'],\n",
    "        normalize=True,\n",
    "        frame_offset=hsn_test[sample]['start_time'] * sr,\n",
    "        num_frames=(hsn_test[sample]['end_time'] - hsn_test[sample]['start_time']) * sr\n",
    "        )\n",
    "sample_audio = sample_audio[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen to the Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "# Play the audio\n",
    "# sr is sampling rate which is 32000 here\n",
    "Audio(data=sample_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the wave form of the Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the time axis\n",
    "time = np.arange(len(sample_audio)) / sr\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(time, sample_audio)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Waveform')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a spectrogram form of the Audio\n",
    "\n",
    "> In this section, \"power_to_db\" function changes the power to dB scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Spectrogram\n",
    "\n",
    "spectrogram_conversion= Spectrogram(n_fft=1024)\n",
    "spectrogram=spectrogram_conversion(sample_tensor[0])\n",
    "spectrogram_db = librosa.power_to_db(spectrogram.squeeze().numpy(), ref=np.max)\n",
    "\n",
    "# Plot the spectrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(spectrogram_db, aspect='auto', origin='lower', extent=[0, len(sample_audio)/sr, 0, sr/2])\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate the Trained Model\n",
    "## Part A: Evaluation with the whole test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model,dataloaders=test_loader,ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Finding the e-Bird codes for a single test sample in multilabel Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PartB_1) Selecting a single sample of test_dataloader\n",
    "\n",
    "> There are two ways for selecting a single sample from your test dataset, the first one is mentioned in step 5 which takes a row audio file, then you need to change its format to spectrogram. We bring the second way here which takes a single sample from dataloader, that is already a spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load the trained model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loader = dm.test_dataloader()\n",
    "batch2 = next(iter(test_loader))\n",
    "# Extract audio data from the batch\n",
    "audiox = batch2[\"input_values\"][0]  # Assuming the first sample in the batch\n",
    "label = batch2[\"labels\"][0]\n",
    "audiox=audiox.unsqueeze(0)\n",
    "print(\"Original audio shape:\", audiox.shape)\n",
    "print(\"Label shape:\", label.shape)\n",
    "# Pass the spectrogram data through the model for prediction\n",
    "with torch.no_grad():\n",
    "    output = model(audiox)\n",
    "print(output.shape,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PartB_2) Prediction of the single sample's labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the model's prediction\n",
    "# This depends on how your model's prediction method is implemented\n",
    "# It could be returning class indices, probabilities, or even class labels directly\n",
    "import torch\n",
    "\n",
    "# Your tensor of logits\n",
    "logits_tensor = output\n",
    "\n",
    "# Apply sigmoid to convert logits to probabilities for each class\n",
    "probabilities = torch.sigmoid(logits_tensor)\n",
    "print(probabilities)\n",
    "# Define a threshold (e.g., 0.5) to determine positive labels\n",
    "threshold = 0.1\n",
    "\n",
    "# Get the predicted labels based on the threshold\n",
    "\n",
    "predicted_labels = (probabilities > threshold).nonzero().squeeze()\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", predicted_labels.tolist())\n",
    "label_indices = [label_set.item() for label_set in predicted_labels]\n",
    "\n",
    "print(\"label_indices\",label_indices)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PartB_3) Mapping of the single sample's labels to e-Birds Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "def get_label_to_category_mapping_from_metadata(\n",
    "    file_path: str, task: str\n",
    ") -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Reads a JSON file and extracts the mapping of labels to eBird codes.\n",
    "\n",
    "    The function expects the JSON structure to be in a specific format, where the mapping\n",
    "    is a list of names located under the keys 'features' -> 'labels' -> 'names'.\n",
    "    The index in the list corresponds to the label, and the value at that index is the eBird code.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the JSON file containing the label to eBird code mapping.\n",
    "    - task (str): The type of task for which to get the mapping. Expected values are \"multiclass\" or \"multilabel\".\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, str]: A dictionary where each key is a label (integer) and the corresponding value is the eBird code.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the file at `file_path` does not exist.\n",
    "    - json.JSONDecodeError: If the file is not a valid JSON.\n",
    "    - KeyError: If the expected keys ('features', 'labels', 'names') are not found in the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the file and read the JSON data\n",
    "    with open(file_path, \"r\") as file:\n",
    "        dataset_info = json.load(file)\n",
    "\n",
    "    # Extract the list of eBird codes from the loaded JSON structure.\n",
    "    # Note: This assumes a specific structure of the JSON data.\n",
    "    # If the structure is different, this line will raise a KeyError.\n",
    "    if task == \"multiclass\":\n",
    "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"names\"]\n",
    "    elif task == \"multilabel\":\n",
    "        ebird_codes_list = dataset_info[\"features\"][\"labels\"][\"feature\"][\"names\"]\n",
    "    else:\n",
    "        # If the task is not recognized (not multiclass or multilabel), raise an error.\n",
    "        raise NotImplementedError(\n",
    "            f\"Only the multiclass and multilabel tasks are implemented, not task {task}.\"\n",
    "        )\n",
    "\n",
    "    # Create a dictionary mapping each label (index) to the corresponding eBird code.\n",
    "    mapping = {label: ebird_code for label, ebird_code in enumerate(ebird_codes_list)}\n",
    "\n",
    "    return mapping\n",
    "\n",
    "mapping = get_label_to_category_mapping_from_metadata(\n",
    "    file_path='../../data_birdset/HSN/HSN_processed_42_467ad9795903cdde/train/dataset_info.json',\n",
    "    task='multilabel'\n",
    ")\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PartB_4) Convert Prediction outputs to Class Label Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_names = [mapping[idx] for idx in label_indices]\n",
    "real_class_names = [mapping[idx] for idx in label]\n",
    "# Print the predicted class names\n",
    "print(\"Real Class Names:\", real_class_names)\n",
    "print(\"Predicted Class Names:\", predicted_class_names) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdset-xS3fZVNL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
