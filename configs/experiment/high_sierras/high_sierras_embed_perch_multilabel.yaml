# @package _global_
#package global is neccessary!
defaults:
  - override /datamodule: high_sierras.yaml
  - override /datamodule/transforms: embedding_transform.yaml
  - override /module: multilabel.yaml
  - override /module/network: detlogreg_perch.yaml
  - override /callbacks: embed_default.yaml 
  - override /trainer: default.yaml


tags: ["mr", "perch", "multilabel", "embedding"]
seed: 42

# module:
#   optimizer:
#     lr: 2e-5

trainer:
  min_epochs: 1
  max_epochs: 128

module:
  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-3
    weight_decay: 0
  loss:
    _target_: torch.nn.BCEWithLogitsLoss
  lr_scheduler: null
  output_activation: 
    _target_: "torch.sigmoid"


datamodule:
  _target_: src.datamodule.embeddings_datamodule.EmbeddingsDataModule
  dataset:
    mode: "local"
    data_dir: ${paths.dataset_path}/embeddings/${module.network.model_name}
    val_split: 0.2
    classlimit: null
    eventlimit: null
  loaders:
    train:
      batch_size: 256
    valid:
      batch_size: 256
    test:
      batch_size: 256
  transforms:
    max_length: 5.0

logger:
  wandb:
    tags: ${tags}
    group: "high_sierras_embed_perch_multilabel_s"
    mode: offline